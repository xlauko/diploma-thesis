\chapter{Preliminaries}\label{ch:preliminaries}

In this chapter, we will introduce foundations for the proposed
abstraction-based model checking approach. We will cover \LLVM bitcode as the
input language for verification. Focus will be on the instruction set for
value manipulation and its semantics, the \LLVM type system and the control flow
of programs. Subsequently, the notion of model checking will be explored in
the context of \LLVM programs. The exposition of model checking is mainly based on
Petr Ročkai's dissertation \cite{Rockai15} and is further covered in \autoref{ch:divine}.

\section{Intro to \LLVMIR} \label{sec:introtollvm}
\marginpar{The \LLVM project is a collection of modular and
reusable compiler and toolchain technologies. Despite its name -- low level
virtual machine, \LLVM has little to do with traditional virtual
machines. The name \LLVM itself is not an acronym; it is the full name
of the project. \href{http://llvm.org/}{[llvm.org]}}

Besides defining the semantics of bitcode in the form of \LLVMIR (intermediate
representation), \LLVM{}~\cite{LLVM:web} is primarily a toolkit for building
compilers. It was first introduced by Ch. Lattner and V. Adve \cite{Lattner04}
as an optimization framework. Nowadays, \LLVM represents a set of libraries for
building and manipulation of an intermediate representation for compilers and
platform-specific code generators.

According to \LLVM language reference manual~\cite{LLVM:langref},
\LLVM is a static single assignment (\SSA) based representation that
provides type safety, low-level operations, flexibility, and
the capability of representing ‘all’ high-level languages cleanly. It is the
common code representation used throughout almost all phases of the \LLVM compilation
strategy.

Since the \LLVMIR simplifies semantics of the verified language, it is much more
suitable for model checking. Model checking can abstract from complicated
grammar of the verified language and process \LLVM bicode produced by the compiler
front-end.

\subsection{Instructions, Functions and other LLVM building blocks}
\label{subsec:infnllvm} \LLVMIR aims to be slightly richer assembly like
language. When we look into a \LLVMIR file (or \emph{module}\sidenote{\LLVM
programs are composed of modules, each of which is a translation unit of the
input programs.}), we find a description of global data and function
definitions. Each function definition consists of a control flow graph formed by
basic blocks.

\begin{definition}\label{def:basicblock}
A basic block is a sequence of instructions that ends with a terminator
instruction (such as a branch or function return). The computation of a program
within a basic block is entirely sequential, performing instructions in their
order of appearance in the block~\cite{Rockai15}.
\end{definition}

Instructions from basic blocks manipulate values in virtual registers or
they move values between registers and memory. Each instruction has at most one
return value, which is assigned to a register for the time of execution of a
function. Considering values in registers, \LLVM bitcode is always in a
partial \SSA (static single assignment) form.

\begin{definition}\label{def:ssa}
Static single assignment is a property of an intermediate representation, which
requires that each variable is assigned exactly once, and every variable is
defined before it is used~\cite{Cytron91}.
\end{definition}

In addition to register values \LLVMIR allows address-taken variables (i.e. it's
allowed to take the address of a variable and mutate its value).
Hence, address-taken variables are not part of the \SSA form. In unoptimized
\LLVM bitcode, each higher-level (C/\Cpp{}) variable becomes an address-taken
variable, created by an \code{alloca} instruction. Consequently, register values are only
created by the compiler as intermediate results of instructions. However, the \LLVM
optimiser can lift many address-taken variables into registers if the addresses
of those variables are not actually taken.

For our purpose we will meet only a subset of \LLVM instructions. Concretely
instructions for manipulation of transformed values, memory operations and
control flow (terminator) instructions. Precise documentation of all can be
found in the \LLVM language reference manual \cite{LLVM:langref}. For better
understanding of \LLVM bitcode, please look at the following example.

\begin{example} \label{ex:factorial}
shows an implementation of \code{factorial} in \Cpp{} and its corresponding \LLVM bitcode.

\begin{minted}{cpp}
int factorial(int n) {
    if (n == 0 )
        return 1;
    return n * factorial(n - 1);
}
\end{minted}

\begin{minted}{llvm}
define i32 @factorial(i32 %n) {
entry:
  %1 = icmp eq i32 %n, 0
  br i1 %1, label %then, label %else
then:
  ret i32 1
else:
  %2 = sub nsw i32 %n, 1
  %3 = call i32 @factorial(i32 %2)
  %4 = mul nsw i32 %n, %3
  ret i32 %4
}
\end{minted}
\end{example}

\autoref{ex:factorial} demonstrates a code of \code{factorial} function. In
\LLVM, the \code{factorial} function is defined as a global object (prefixed by
\code{@}) \marginpar{To distinguish between register values and mutable
address-taken variables \LLVM use two types of prefixes, \code{\%} for register
values and \code{@} for mutable and global values.} \code{@factorial}, with
return type \code{i32} (a 32 bit integer) and an argument called \code{\%n},
also of type \code{i32}. In the function implementation, we can see almost
direct correspondence to \Cpp{} code. The function is logically divided into a
few basic blocks -- \code{entry}, \code{then} and \code{else}
(see~\autoref{def:basicblock}). First basic block, also called \emph{entry}
block, contains equality comparison (\code{icmp} instruction) of the input
argument \code{\%n} and \code{0}, followed by a conditional branching
instruction \code{br}.

The branching instruction connects basic blocks and determines control flow
depending on the value of register \code{\%1}. Branching jumps to basic
block labeled with \code{\%then} if the value \code{\%1} is true,
otherwise it jumps to \code{\%else} basic block. Besides branching, \LLVM
uses \code{br} instructions to represent loops, by jumping backwards in the
control flow graph.

In the basic block \code{\%then}, value 1 is returned with \code{ret}
instruction. Further, in the \code{\%else} branch, besides arithmetic operations
(\code{sub} representing subtraction and \code{mul} multiplication), a
recursive call is made using a \code{call} instruction.

\subsection{Type system} \label{subsec:typesystem}

An observant reader may have noticed that \LLVMIR is a statically typed
language.  Besides the \code{void} type, \LLVM distinguishes basic single value
types, such as \code{iN} where \code{N} determines the bit width of an integral
type. We have already met \code{i32} as 32-bit integer and \code{i1} usually
used for booleans.  \LLVM additionally supports floating point types as
\code{float} and \code{double} and also pointer types written as \code{<type>*}.

In order to support aggregate types, \LLVM recognizes array types, structure types
and vector types \cite{LLVM:langref}. Similarly as in C, array type represents a
contiguous block of memory, e.g.~to denote array of 10 integers
\LLVM writes \code{[10 × i32]}. To represent a collection of data members
together in memory, \LLVM defines a structure type. \LLVM identifies structure types
by given identifiers in the global namespace as follows. For a structure called
\code{aggregate} consisting of \code{i32} and a pointer to pair of \code{i32}
values:

\begin{minted}{llvm}
%aggregate = type { i32, { i32, i32 }* }
\end{minted}

Structures in memory are accessed using \code{load} and \code{store} instructions.
A \code{getelementptr} instruction is used to compute an offset into an aggregate type.
Structures in registers are accessed using the \code{extractvalue} and \code{insertvalue}
instructions \cite{LLVM:langref}.

In order to cast between types, \LLVM provides a variety of casting instructions.
To be specific, a \code{bitcast} instruction for explicit type casting, \code{ptrtoint} and
\code{inttoptr} for conversions between pointers and integers and \code{trunc}, \code{zext},
\code{sext} for transformation to larger or smaller integer types.

\subsection{Control flow} \label{subsec:controlflow}

As mentioned earlier, \LLVM supports control flow instructions like \code{br} for
conditional and unconditional jumps between basic blocks. To direct control flow
between functions, \LLVM provides \code{ret} for returning from functions, \code{call} and
\code{invoke} for calling a function. Except \code{call}, all control flow instructions
are always at the end of a basic block, hence they are called \emph{terminator}
instructions.

As consequence of branching in the control flow graph and of its \SSA form, \LLVM needs to
support acquisition of a value from different basic blocks. For this
purpose, \LLVM contains a special instruction -- a \code{phi} node \cite{Cytron91}.

\begin{minted}{llvm}
%v = phi i32 [ %a, %then ], [ %b, %else ]
\end{minted}

As a result of a \code{phi} node, \code{\%v} contains either the value \code{\%a},
if the execution came from basic block \code{\%then}, or the value \code{\%b} for execution
coming from the \code{\%else} basic block.

\subsection{Semantics} \label{subsec:semantics}

In order to build abstraction over \LLVM bitcode, we need to set up a good
semantic model of the instruction set. For this purpose, small-step operational
semantics \cite{Plotkin04} is a good candidate.

\begin{definition}
Operational semantics express the evaluation of commands as a relation between
a command, an initial state and a final state.
\end{definition}

\noindent
Since \LLVMIR semantics prevents unnamed value creation, we don not need to
bother with expression semantics. The only state mutators are instructions, which are
semantically atomic and operate only on existing named values. Moreover,
instructions may mutate at most a single value in the program state. Hence, the
semantic function with derivation rules can be derived from \LLVM
language reference \cite{LLVM:langref}. Program state will basically
consists of two parts -- stack frames and memory. Stack frames represent
register values, while the memory portion stores allocated variables.
Further description of \LLVM semantics as used in \DIVINE can be found in Ročkai's
dissertation thesis \cite{Rockai15}.

\section{\LLVM-based compilation process} \label{sec:compilation}

The compilation process using \LLVM starts with a \emph{front-end} compiler that produces
\LLVMIR from higher-level code, e.g.~\clang for \Cpp{}, see \autoref{fig:compilation}.
Bitcode generated by the front-end is then processed and optimized by \LLVM.
Optimization is divided into \LLVM \emph{passes}, where each of the passes
performs some optimization task or code analysis. Passes can be run directly
in the compiler or they can be run by an external tool, e.g.~the \LLVM
optimization tool \emph{opt} \cite{LLVM:opt}. After optimization, \LLVMIR is passed to
an appropriate code generator, which generates assembly for a specific
platform, e.g.~\texttt{x86} or ARM.

\begin{figure}[!ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth,shorten >=1pt,auto,node distance=4em, <->]

    \tikzstyle{component}=[draw, color=pruss, text centered, rounded corners=1pt, minimum height=2.8 em, minimum width=2 cm, text width=1.8 cm]
    \tikzstyle{inout}=[color=pruss]
    \tikzset{>=latex}
    \tikzset{font={\fontsize{9pt}{12}\selectfont}}

    \node [component] (clang) { Clang \Cpp{} front-end};
    \node [component, below = 0.5 cm of clang] (gcc) { llvm-gcc front-end};
    \node [component, below = 0.5 cm of gcc] (ghc) { GHC front-end};
    \node [inout, left = 0.4 cm of clang](c++) {\Cpp};
    \node [inout, left = 0.4 cm of gcc](fortran) {Fortran};
    \node [inout, left = 0.4 cm of ghc](haskell) {Haskell};

    \node [inout,component, right = of gcc] (opt) {\LLVM optimizer};

    \node [component, right = of opt] (power) {\LLVM PowerPC back-end};
    \node [component, above = 0.5 cm of power] (x86) {\LLVM \texttt{x86} back-end};
    \node [component, below = 0.5 cm of power] (arm) {\LLVM ARM back-end};

    \node [inout, right = 0.4 cm of x86] (rx86) {\texttt{x86}};
    \node [inout, right = 0.4 cm of power] (rpower) {PowerPC};
    \node [inout, right = 0.4 cm of arm] (rarm) {ARM};

    \draw [flow] (c++) -- (clang);
    \draw [flow] (fortran) -- (gcc);
    \draw [flow] (haskell) -- (ghc);

    \draw [flow] (clang.east) -| ([xshift= 0.3 cm]gcc.east) -- (opt.west);
    \draw [flow] (gcc) -- (opt) node [inout, pos=0.6,above] {\small \LLVM};
    \draw [flow] (ghc) -| ([xshift= 0.3 cm]gcc.east) -- (opt);

    \draw [flow] (opt) -- ([xshift= -0.3 cm]power.west) |- (x86);
    \draw [flow] (opt) -- (power) node [inout, pos=0.4,above] {\small \LLVM};
    \draw [flow] (opt) -- ([xshift= -0.3 cm]power.west) |- (arm);

    \draw [flow] (x86) -- (rx86);
    \draw [flow] (power) -- (rpower);
    \draw [flow] (arm) -- (rarm);
\end{tikzpicture}
}
\caption{\LLVM compilation process consists of 3 steps -- parsing of the input
language and producing \LLVMIR in the front-end part, optimization of produced
\LLVMIR and generation of platform specific assembly by the back-end.}
\label{fig:compilation}
\end{figure}

\section{Model checking} \label{sec:mc}
Model checking is an automated technique that, given a finite-state model of a
system and a formal property, systematically checks whether the property holds
for the model \cite{Baier08}. To define a model checking problem,
we need to set up a transition system that describes the system to be
verified. For our purposes, we will talk about transition-based model checking,
which checks satisfaction of the property on the transitions of the system.

\noindent
\begin{minipage}{\textwidth}
\begin{definition}\label{def:ts}
\medskip
A transition system $TS$ is a tuple $(S, s_0, Act, T)$, where:
\begin{itemize}
    \item $S$ is a set of states,
    \item $s_0 \in S$ is an initial state,
    \item $Act$ is a set of actions,
    \item $T \subseteq S \times Act \times S$ is a
    transition relation.
\end{itemize}
\end{definition}
\end{minipage}
\noindent
We denote a transition from state $p$ to $q$ under the action $a$ as $p
\xrightarrow[]{a} q$. In model checking of \LLVM programs, state represents a
memory configuration and values of stack variables. Transition from one state to
another is then induced by evaluation of \LLVM instructions (they modify state
variables or introduce a new one).\sidenote{For example, an instruction
\code{\%c = add \%a \%b} introduces a new register value \code{\%c}, hence
modifies the state.}

Given a transition system, we define a run $\varrho$ as an alternating
sequence of states and actions starting from $s_0$ \cite{Baier08}:
\[
    \varrho = s_0 a_1 s_1 a_2 s_2 a_3 \dots \text{ such that } s_i
    \xrightarrow[]{a_{i+1}} s_{i+1} \text{ for all } 0 \leq i.
\]

We will mark transitions by two types of actions -- $acc$ for an accepting edge,
and $err$ for an erroneous edge. One edge may contain both markings or neither of
them, hence the set of all possible actions is $Act = \mathcal{P}(\{acc,
err\})$.

We employ error markings in verification of safety properties~\cite{Manna92}.  If a
sequence of instructions corresponding to an edge produces an error (null
pointer dereference, violation of an assertion, etc.), we mark the edge by the
$err$ action. When such an edge is seen during verification, the model checker
declares an error. To verify a program we need to prove that an error transition
is never reachable. This is achieved by exhaustive exploration of the transition
system $M = (S, s_0, Act, T)$ of the program. To be precise we check whether all
runs starting from $s_0$ do not contain $err$ mark on any of their edges.

Besides safety properties, we may want to verify temporal properties
\cite{Manna92}. Temporal properties can describe not just properties about
edges, but also properties about runs of a program. For example, we can check
that variable $x$ will be eventually greater then 42. To verify such a property
an automata-theoretic approach is used \cite{Vardi96}. In this approach, we have
to denote accepting edges by $acc$ mark. We say that a run is accepting if it
contains infinitely many accepting edges. To check whether a program satisfies a
temporal property $\varphi$, we do not have to decide whether all runs starting
from $s_0$ satisfy $\varphi$, but we can only check whether there does not exist
a run satisfying~$\neg \varphi$. To do so, we check whether there exists an
accepting run for property $\neg \varphi$, if not then $\varphi$ hold for the
program.

Further in thesis we will use a model checker called \DIVINE. It is based on
transition-based model checking and applies an automata-theoretic approach
\cite{Vardi96}. In automata-based model checking we get a program $P$
and temporal property $\varphi$ to be verified (in our case in the \LTL logic).
We create a transition system $M$ of the program $P$, and construct a
transition-based automaton $A_{\neg \varphi}$ representing the negation of the
property~$\varphi$. Model checking algorithm is then executed on the product of
$M$ and $A_{\neg \varphi}$. The whole process is further described by
A.~Duret-Lutz \cite{Duret04}. When $M$ satisfies property $\varphi$, we write
$M \models \varphi$.

\subsection{Abstraction-based model checking} \label{subsec:amc}

When the number of states is too large, it may be difficult to determine whether a
program is correct. To reduce the size of the state space, a technique called
abstraction-based model checking was proposed by E. Clarke et al.~\cite{Clarke94}.

In abstraction-based model checking, we abstract a transition system, replacing
concrete states by a suitable abstract representation. In this way, we can
represent a set of concrete states by a single abstract state. But we require
that if there was a transition between two concrete states, there has to be a
transition under same action between the abstract states that represent the
concrete ones.

Formally, given a concrete domain\sidenote{For example all possible values
in~\LLVM.} $C$, we define a concrete state as an $n$-tuple of concrete values:
$s \in C^n$, where each element represents a value of a program variable (there
are $n$ variables in the program location of state $s$). An abstract state is
then an $m$-tuple of abstract values from an abstract domain~$A$. A
transformation from a concrete state to an abstract state is defined by a
surjection $\alpha \colon C^n \rightarrow A^m$. Note that the abstract state can be
represented by a different number of values than the concrete state.
\begin{example}
In \LLVM we can have a concrete state $s$ with two variables $x$
and $y$: $s = (x:5,y:0)$. Let's have an abstraction that determines whether a state
contains a value equal to zero.\sidenote{This is a type of a predicate
abstraction~\cite{Flanagan02}.} Then abstract abstract state $\widehat{s}$ contains only
a single value $true$.
\end{example}

To get an abstract transition system from a transition system $M$, we need to
transform concrete states to abstract states and rearrange the transitions
accordingly. Intuitively, we want an abstract state $\widehat{s}$ to represent
all states from $M$ for which $\alpha(s) = \widehat{s}$.

\begin{definition}\label{def:am}
    Let $\widehat{M}$ be an abstract transition system. We say that $\widehat{M}$
    over-approximates $M$ when
    \begin{enumerate}
        \item $\alpha(s_0) = \widehat{s}_0$ is the initial state of $\widehat{M}$, and
        \item for each transition $s \xrightarrow[]{a} p$ in $M$  there is a
            transition\\$\alpha(s) \xrightarrow[]{a} \alpha(p)$ in $\widehat{M}$.
    \end{enumerate}
\end{definition}
Remark that we maintain actions of transitions. This has to be done in order
to preserve soundness of the method (i.e. when there was an error transition in
the original transition system, we want to have an error transition in the
abstract system between the corresponding abstract states).

\begin{example} \label{ex:errtrans}
Performing an abstraction on the transition system on the left, we group a transitions from
original states $s_1$ and $s_2$.
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[->,>=stealth']
    \tikzset{state/.style = {
        circle,
        fnode,
        fill=apple!40,
        minimum width=1.5cm,minimum height=1.5cm
    }}
    \node [state] (s1) {$s_0$};
    \node [state, below = 1 cm of s1] (s2) {$s_2$};
    \node [state, right = 1.5 cm of s1] (s3) {$s_3$};

    \begin{pgfonlayer}{background}
        \node [fun, label = {$M$}, fit = (s1) (s2) (s3)] (sp) {};
    \end{pgfonlayer}

    \node [state, right = 2 cm of s3] (sw1) {$\{s_0, s_2\}$};
    \node [state, right = 1.5 cm of sw1] (sw2) {$\{s_3\}$};

    \draw[flow] (s1) -- (s3) node [midway, above = 1pt] {$\emptyset$};
    \draw[flow] (s2) -- (s3) node [midway, below = 1pt, sloped] {$\{err\}$};
    \draw (sw1) edge[out=35, in=145, flow] node [midway, below = 1pt] {$\emptyset$} (sw2);
    \draw (sw1) edge[out=-35, in=-145, flow] node [midway, below = 1pt,name=err] {$\{err\}$} (sw2);

    \begin{pgfonlayer}{background}
        \node [fun, label = {$\widehat{M}$}, fit = (sw1) (sw2) (err)] (asp) {};
    \end{pgfonlayer}

    \draw[flow, very thick, shorten >= 7pt, shorten <= 7pt] (sp.east |- asp.west) -- (asp.west) node [midway, above = 1pt] {$\alpha$};
\end{tikzpicture}
}
\end{center}
\end{example}

Once we have an abstract transition system $\widehat{M}$, model checking is
straightforward. Exploration of the abstract state space is done in the same
manner as in the original explicit-state model checking approach. The only
difference is that the satisfaction of a property has to be checked over
abstract transitions. A problem occurs when the abstraction makes accessible an
error that was unreachable in the concrete system (see an
\autoref{ex:errtrans}).
In this case a false positive error is found. However, since the abstracted system
includes all the behaviours of the original model $M$, we know that if
$\widehat{M} \models \varphi$ then $M \models \varphi$, for some property
$\varphi$. If the abstraction satisfies also the property $\widehat{M}
\not\models \varphi \implies M \not\models \varphi$, we say that the abstraction
is \emph{precise}.
