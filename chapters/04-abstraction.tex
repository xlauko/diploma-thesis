\chapter{Program abstractions}\label{ch:abstraction}

As we have already seen, abstractions are applicable in many ways for model
checking and in verification in general. In this chapter we will propose a novel
approach for verification of programs with inputs. We aim to provide a tool to
enable abstraction of inputs, with only minor modifications to the model
checker.

One of the approaches, as mentioned in \autoref{ch:divine}, is symbolic model
checking. We may look on the symbolic representation as on form of abstraction,
since input data are abstracted into the sets of values. In this chapter, we will
review how programs are abstracted more generally. Firstly we will introduce
simpler abstract domain, which does not require that much technicalities as
the symbolic abstraction. Thereafter we will present an approach to abstractions via
program transformation. At the end of the chapter we explore how the transformed
program with the symbolic abstraction interacts with \DIVINE.

\section{Approaches to abstraction}

In general there are two main approaches how to do abstraction of programs.  A
traditional way of abstraction engines is to interpret the given
program~\cite{Cousot79}. During interpretation, the abstraction tool needs to
interpret code according to some semantics (either encoded in the tool or
given). Also \SymDIVINE works in similar manner. During the interpretation of \LLVM
bitcode \SymDIVINE manipulates a formula representing a multi
state, see \autoref{sub:symdivine}.

The other alternative to do abstraction is to compile the abstraction into the
program. It means that the program will be transformed in such manner that it will
manipulate with abstract values instead of concrete ones. For example, in the
case of symbolic abstraction, the abstracted program manipulates directly
formulae, instead of concrete variables. The abstracted program after such
transformation behaves almost as a traditional program. The main difference is
that the abstracted program may become nondeterministic.

When comparing the above mentioned approaches, each of them comes with its pros
and cons. The transformation approach does not complicate the design of the
verification tool since it is done before the verification process starts, hence
it minimize the probability of creating a bug in the verification tool itself.

Further, the verification over the transformed program checks the correctness of
the inserted abstraction, since it is encoded into the program. This is not
possible in interpretation approach, because the abstraction is part of the
interpreter.

Both approaches enable pluggable abstract domains, by generalizing their
common analysis, as will be shown in this chapter.

On the other hand, the transformation approach requires a more complicated
design, since most of the computation has to be done statically. Also the
verification of transformed program may potentially become slower, since
abstract operations have to be interpreted by the verification tool.

Since \DIVINE is strong a foundation for explicit model checking, we have
decided not to complicate the precisely crafted interpreter (\DIVM), and take
the path of the second approach. We expect the program transformation approach
to generalize the common analysis used for abstraction, hence provide a
toolset that will be easily extended to other abstractions.

Besides having symbolic abstraction as a main goal, we have realized that the
core of transformation is usable for arbitrary abstraction, as will be shown in
this chapter.

\section{Abstract domains} \label{sec:absdom}

When dealing with abstraction of programs, first of all, one has to define an
abstract domain for program variables and operations(transformers) based on
operational semantics of a program. From model checking point of view, we want
to define an abstraction function $\alpha \colon C \to A$, that defines a
transformation from a concrete state $C$ to an abstract state $A$, where $C$
corresponds to some memory valuation (tuple of concrete values) and $A$
corresponds to a tuple representing a state in the abstract world, see
\autoref{subsec:amc}. Additionally we have to define abstract instructions that
manipulates with abstract values (in our case abstract equivalents of \LLVM
instructions).

\bigskip

\hrule

\bigskip
\noindent
\textit{An abstract domain is an abstract algebra, implemented as a library module,
providing a description of abstract program properties and abstract property
transformers describing the operational effect of program instructions and
commands in the abstract program. Abstract domains are often complete lattices.
--- P. Cousot \cite{Cousot79} }
\bigskip

\hrule

\bigskip

In order to define the transformation from a concrete domain to an abstract
domain, we have to describe the concrete domain first. Values of concrete domain
in \LLVM are partitioned according to their types. For simplicity, we will work
only with a subset of \LLVM types -- integer types, pointer types and aggregate
types. Moreover we do not permit abstraction of pointers, since pointer analysis is
beyond the scope of this thesis, but we will permit pointers to abstract values.

We define integer types as a set $C_{i}$ containg all \LLVM integer
types (i.e.~a type for each bitwidth).
Aggregate types $C_a$ are then defined as all finite products of integer types and
pointer types, and set of pointer types $C_p$ consists of pointers to all integer
and aggregate types and pointers to pointers and so on. The whole type system of concrete domain is
then defined as union of these sets: $C = C_i \cup C_a \cup C_p$. We will denote
the set of all the possible concrete values by $V_C$. These are all \LLVM values
of types given by $C$. To set up abstraction of \LLVM, we define an arbitrary
abstract domain $A$ as:
\begin{definition}
An abstract domain $A$ is defined as an ordered tuple $(V_A,\alpha_A, lift, lower,
    T)$ where:
\begin{itemize}
    \item $V_A$ is a set of possible abstract values,
    \item $\alpha_A \colon C \to A$ is conversion from concrete types to
        abstract types,
    \item $\textit{lift} \colon \mathcal{P}(V_C) \to V_A$ defines a function for
        value abstraction, where $\mathcal{P}(V_C)$ denotes a power set of
        $V_C$\sidenote{We remark that elements of $V_C$ are sets of possible
        values.},
    \item $\textit{lower} \colon V_A \to \mathcal{P}(V_C)$ defines a
        transformation from abstract value to concrete values,
    \item $T$ is a set of abstract instructions.
\end{itemize}

\end{definition}

%Since we do not permit abstract pointers, the $\alpha$ function for pointer
%types and will be always defined as transformation from concrete pointer type to
%abstract pointer type with corresponding abstracted base type. Similarly the
%transformation of aggregate types may be done in per element manner. Hence only
%the definition of integer types transformation is interesting. Similar reasoning
%applies to \textit{lift} and \textit{lower} functions, where only transformation
%of the integer values is needed to by specified.

As an example of a simple domain, we define a three-value domain $Z$ where only
one type exists with following possible values:
\[ V_{Z} = \{ \textit{zero}, \textit{nonzero}, \textit{unknown} \}\]


The values represent whether the abstracted variable is zero, non-zero or that
we do not know anything about the value of the variable (\emph{unknown}). Since
we have only one type $\alpha_{Z}$ maps all integer types to single possible
type.  The definition of $\textit{lift}$ and $\textit{lower}$ for integer types
in the domain $Z$ is straightforward:

\[
  lift(c) =
  \begin{cases}
    \textit{zero}    & \text{if } \{0\} = c, \\
    \textit{nonzero} & \text{if } 0 \not \in c, \\
    \textit{unknown} & \text{else.}
  \end{cases}
\]

\noindent
Because lower depends on resulting bitwidth of \LLVM integer type we define separate
function for each bitwidth ($b$):

\[
  lower_{b}(a) =
  \begin{cases}
    \{0\}      & \text{if } a = \textit{zero},\\
    \{1, 2^b\} & \text{if } a = \textit{nonzero}, \\
    \{0, 2^b\} & \text{else}. \\
  \end{cases}
\]

The abstract instructions $T_Z$ correspond to \LLVM instructions, but they
manipulate with values from $V_Z$. For example, abstract instruction \emph{mul}
has type $V_Z \times V_Z \rightarrow V_Z$, and it defines a multiplication in
domain $Z$:

\[
  mul_Z(a, b) =
  \begin{cases}
    \textit{zero}      & \text{if } a = \textit{zero} \text{ or } b = \textit{zero},\\
    \textit{nonzero}   & \text{if } a = \textit{nonzero} \text{ and } b = \textit{nonzero}, \\
    \textit{unknown}   & \text{else}. \\
  \end{cases}
\]

Our main goal is to allow the domain to be implemented in some common higher-level
language and consequently compiled into the verified program. For this purpose,
we have chosen \Cpp{} language. To represent the $Z$ domain in
\Cpp{} we have created a simple enumeration type:
\begin{minted}{cpp}
struct Zero {
    enum Domain { ZeroValue, NonzeroValue, Unknown }
    Domain value;
};
\end{minted}
Domain defined in this way, can be then compiled and linked with a verified
program. In compiled code abstraction leverages the existing type system of
\LLVM, since the compiled definition of abstraction is encoded by \LLVM scalars.

To cover all the \LLVM instructions that can manipulate with abstract values we
need to implement binary arithmetic operations, bitwise binary operations,
memory accesses, conversion operations and comparison instructions.

Implementation of abstract instructions may be delivered as \Cpp{} functions over
the corresponding abstract types. For example, an abstract equivalent of the
\code{mul} instruction in the domain $Z$ may be implemented as follows.\footnote{The whole
\code{zero} domain implementation can be found in thesis attachments, with
only minor technical differences. Code in the text is cleaned for a readability.}

\bigskip
\noindent
\begin{minipage}[bt]{\fullwidth}
\begin{minted}{cpp}
Zero __abstract_zero_mul( Zero a, Zero b ) {
    if ( a.value == Zero::Domain::ZeroValue || b.value == Zero::Domain::ZeroValue )
        return Zero::Domain::Zero;
    if ( a.value == Zero::Domain::NonzeroValue && b.value == Zero::Domain::NonzeroValue )
        return Zero::Domain::Nonzero;
    return Zero::Domain::Unknown;
}
\end{minted}
\end{minipage}
\bigskip

Generally the binary bitwise, arithmetic and comparison operations are of
type $V_Z \times V_Z \to V_Z$. Moreover we define coinversion operations of type
$V_Z \to V_Z$,
and operations for memory accesses \code{alloca}, \code{store} and \code{load}. The abstract \code{alloca} instruction allocates a new \code{Zero}
variable and returns a pointer to it. The \code{load} and \code{store} access memory as expected.

\subsection{Domain interactions} \label{sec:interactions}
In cases when different concrete domains interact, special care needs to be
taken. We need to solve two major types of interactions. First one occurs
with casting operations (bit-casting, width extension, truncation, etc.), when
values are transformed from one domain to another. The second
interaction between domains occurs during comparisons, where \LLVM produces from
a predicate over various values a boolean result. On the boolean result
usually depends some branching, which needs to take concrete
value.\sidenote{Since we aim for control explicit verification, we can not
abstract branching instructions.} Hence the
abstract result from comparison needs to produce a concrete value. But since the
result of the comparison may be uncertain\sidenote{In the domain $Z$, this may happen when the
comparison result is an \emph{unknown} value.}, we need to do a nondeterministic
choice to produce both \emph{true} or \emph{false}.

To unify this interaction with control flow between domains, we introduce a
tristate domain (\emph{true, false, maybe}), and require all abstract domains to
define an operation that takes their abstract boolean and transform it to the
tristate domain. We will refer to it as \code{bool\_to\_tristate}.  Hence before
each branching on an abstract value we need to transform it to generalized form
of tristate and according to value of the tristate decide whether to do the
nondeterministic choice.\sidenote{Nondeterministic choice is made only when the
tristate has a value \emph{maybe}.} The abstract control flow will be further
described in \autoref{subsec:cf}.

Handling of cast interactions is much more tricky, generally a combinatorial
amount of code is required for transformation between abstract domains. The
simplest way is to treat bit-casts as a way of aliasing and enforce both
variables to be abstracted into the same abstract domain \cite{Rockai15}.


\subsection{Control flow of abstract programs} \label{subsec:cf}
When a branch is taken due to a \emph{maybe} result of some boolean expression,
the effect of the branch can be, in addition to its usual concrete semantics,
to restrict the values of some abstracted variables. Abstract interpreters
naturally take advantage of this fact, by building up a path condition
\cite{Rockai15}.

In a transformation-based approach, we can take advantage of the same principle, by
restricting the values on which the branch has been taken. For this purpose, we
require an abstract domain to provide an \code{assume} operation. The
\code{assume} operation has to be able to compute a new value for the constrained
variable from given a \LLVM predicate, such as \code{icmp}, and the actual result of
that predicate (i.e.~\emph{true} or \emph{false}). Control flow of abstract
programs is further examined in \autoref{sec:bcp}.

\subsection{Symbolic domain}\label{sec:sym}

In comparison to the simple domain $Z$, the symbolic domain brings a few
complications. First of all, we need to create a representation of symbolic
data, and implement required operations. In the \SymDIVINE algorithm, the data were
described by two parts: \emph{data definitions} and \emph{path condition} (see
\autoref{sub:symdivine}). We would like to bring this notion of separation of
data and path condition also to the abstracted program.

In \SymDIVINE data definitions were created by the instructions. In abstracted
program we will do the same, i.e.~the symbolic definitions are will be created either via symbolic
\code{alloca} or as a result of symbolic instruction.

The path condition should reflect the already taken path in the program, hence
it will be described by assumes that were encountered during the interpretation
(i.e.~constraints given by branching).

Symbolic data needs to represent a bit-vector formula, that defines their data.
A common formula representation are formula trees. For example in the following
program, the data representation at the end of the program by formula trees will
be as on the left picture.

\bigskip
\noindent
\adjustbox{valign=t}{
\begin{minipage}{.4\textwidth}
\begin{minted}{cpp}
int x = input();
int y = input();
int a = x + y;
int b = a * x;
\end{minted}
\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}{.55\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm/#1, level
    distance = 1cm}]
    \node [fvar] (x) {$x$};
    \node [fvar, right = of x] (y) {$y$};
    \node [fvar, right = of y] (a) {$a$};
    \node [fvar, right = of a] (b) {$b$};
    \node [fop, below = 0.6cm of a] (plus) {$+$};
    \node [fop, below = 0.6cm of b] (times) {$*$};
    \node [fatom, below = 1cm of x] (xa) {};
    \node [fatom, below = 1cm of y] (ya) {};

    \draw [flow] (x) -- (xa);
    \draw [flow] (y) -- (ya);
    \draw [flow] (a) -- (plus);
    \draw [flow] (b) -- (times);
    \draw [flow] (plus) to [out=-135, in=-45] (xa);
    \draw [flow] (plus) to [out=-135, in=-45] (ya);
    \draw [flow] (times) to [out=-135, in= -45] (plus);
    \draw [flow] (times) to [out=-135, in= -45] (xa);
\end{tikzpicture}
\end{minipage}}

The trees are induced from the \SSA form of the program --- see that each
variable points to the root of its formula tree. The input values $x$ and $y$
point to the nodes representing atoms the formula, i.e.~arbitrary values.
However, $a$ and $b$ point to nodes that represents on the abstract values.
Overall, the operations in the symbolic domain are defined as operations on
the tree representation.

\begin{summary}
In this section we have defined what is an abstract domain in the context of program
transformations. We have established a representation of the abstract domain in
the high-level programing language, accompanied with abstract operations. In
addition to abstract \LLVM instructions we have introduced \code{lift} and
\code{lower} for conversion between the concrete and the abstract domain. We have
examined interactions between domains, as casting between domains and
transformation to tristate domain. Consequently we have analyzed the control flow
of abstracted program, that introduces nondeterministic choices into the
branching. Lastly we have settled a representation of the symbolic domain via formula
trees.
\end{summary}

\section{Abstraction via program transformation}

In following sections we will describe the actual implementation of domain
insertion into the program. By providing a common set of analyses, we aim for
much easier implementation of new abstract domains. Furthermore, we want to
create the transformation in such a manner, that the resulting program will be a
runnable binary, provided the nondeterministic choice implementation.\sidenote{To
implement nondeterministic choice for standalone program one can use a standard random
function or a bit-vector of choices} Hence the abstracted program should be
analyzable by arbitrary tools, not just by \DIVINE. Moreover to preserve
the soundness of further analysis we have to define the transformation in an
information-preserving manner.

Ergo, we have chosen the transformation to be done on the \LLVM bit-code level to
mimic the logic of the original program as precisely as possible. The whole
transformation is done in successive standalone \LLVM passes, producing the
abstracted \LLVM bitcode. It is further described in following sections, and put
together in \autoref{fig:transformation}.

\begin{figure}[!ht]
\checkoddpage
\edef\side{\ifoddpage l\else r\fi}
\makebox[\textwidth][\side]{
\begin{minipage}[bt]{\fullwidth}
\prule
\bigskip

\resizebox{\fullwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}

\tikzstyle{every node}=[align=center, minimum width=1.25cm, minimum height=0.6cm]
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm, text width = 2
    cm, color = pruss}}
\tikzset{pass/.style = {
    draw,
    inner sep=3pt,
    rounded corners= 3pt,
    minimum width = 0cm,
    minimum height = 1.5cm,
    text width = 3cm,
    node distance = 4cm,
    text centered,
    color=pruss,
    fill=pruss!10
}}

\node [pass] (VPA) {Value\\propagation};
\node [pass, right of = VPA] (AI) {Abstraction of instructions};
\node [pass, right of = AI] (BCP) {Constraint propagation};
\node [pass, fill=vivid!40, right of = BCP] (DI) {Domain insertion};

    \node [empty, left = 0.5 cm of VPA]  (in) {bitcode \\ + \\ domains};
\node [empty, right = 0.5 cm of DI]  (out) {abstracted bitcode};

\draw [flow] (in) -- (VPA);
\draw [flow] (VPA) -- (AI);
\draw [flow] (AI) -- (BCP);
\draw [flow] (BCP) -- (DI);
\draw [flow] (DI) -- (out);
\end{tikzpicture}
}

\caption{Sequence of transformation passes used for insertion of an arbitrary
    abstraction.}
\label{fig:transformation}
\bigskip
\prule
\end{minipage}}
\end{figure}

Abstraction of the program is done in four major steps. First, we analyze
wherever abstract values in program may occur. This is done in value propagation
analysis. The computed set of instructions is then abstracted into intermediate
\LART instructions. Thereafter, \code{assume} calls are inserted into the
program and constraints are propagated through the bitcode. Finally the
intermediate abstract representation is replaced by the appropriate operations
in the respective domain. First three analysis are domain independent and can be
done without need of the actual domain implementation.

Providing the program and the domain implementation, one has
to annotate which variables should be abstracted into which domain. In
following example, the variable \code{x} is annotated as symbolic.
\begin{minted}{cpp}
int foo(int a, int b) {
    _SYM int x;
    int y = 0;
    if ( x < a ) {
        y = x + 4;
    } else {
        y = bar(a, b);
    }
    y = bar(a, y);
    return y;
}
\end{minted}
In the next sections we will work with this simple \Cpp{} program. For
readability we will not expose the corresponding \LLVM bitcode and will instead
work schematically on control flow diagrams, on which the mapping to actual
\LLVM should be easily done. But for a curious reader, we attach results of
\LLVM transformation to the \autoref{ch:appendixa}. The corresponding control
flow graph for the example program is displayed in \autoref{fig:exampleprogram}.

In the program with annotations of abstract variables, we need to examine all the
reachable instructions from those variables. We will call them the
\emph{roots} of the abstraction. The examination of the reachable instructions is
done in the first step by \emph{value propagation analysis}~(VPA).

In order to generalize the process and to enable additional analysis, we
postpone abstract domain insertion until the entire program is analyzed and
transformed to a suitable form.

Hence in the second step (after VPA), we introduce an intermediate representation of
abstract instructions. These instructions do not provide any implementation and
are represented a manner similar to \LLVM intrinsic functions
\cite{LLVM:langref}. During this phase (\emph{abstraction of instructions}) we
\emph{lift} the reached instructions provided by the VPA into the abstract form. The
only purpose of these intrinsics is to carry the domain information for further
analysis and to eliminate analysis of the domain implementation (since there is no need
to look into implementation of domain operations). Additionally the \LLVM
toolset can be used in this step for type checking of the applied transformation.

In the next analysis, \code{assume} calls are inserted and backwards constraint
propagation is executed. After all these analyses, the intermediate
representation of abstract operations is replaced by a real domain
implementation.

\begin{figure}[!ht]
\begin{centering}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {\_SYM i32 x };
    \node [llvm, below = 0.5 cm of declx] (decly) { i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [llvm, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [llvm, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [llvm, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5 pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\end{centering}
    \caption{Control flow graph of the example program. The gray
    boxes correspond to \LLVM basic blocks and the yellow diamond to conditional branching
    using the corresponding boolean expression.}

    \label{fig:exampleprogram}
\end{figure}

\subsection{Value propagation analysis}
In the first \LLVM pass, we analyze the reachable instructions from the
annotated variables. Reachable instructions are those that use abstract values
as their arguments. In this analysis, we also need to propagate through calls,
when abstract values are arguments of called function. Similarly analyze
functions into which we can return an abstract value from some function. The
result of VPA is a list of functions with corresponding abstract roots.

\noindent
\checkoddpage
\edef\side{\ifoddpage l\else r\fi}
\makebox[\textwidth][\side]{
\noindent
\begin{minipage}[bt]{\fullwidth}
\begin{example}\label{ex:vpa}
    \bigskip
    When analyzing the function \code{foo} from \autoref{fig:exampleprogram}, we
    compute the reachable set of instructions using DFS from root \code{x}
    through all its uses \cite{LLVM:langref}. We investigate all instructions
    where x appears on the right hand side (marked green on the left picture).
    Since the analysis found a \code{store} (i.e.~\code{y = x + 4}) to variable
    \code{y}, \code{y} is added to set of roots of the function \code{foo} and
    lifted into the domain that $x$ belongs to. Further the analysis is repeated
    from the new root.

\noindent
\bigskip
\resizebox{0.49\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm, fill = apple!40] (declx) {\_SYM i32 x };
    \node [llvm, below = 0.5 cm of declx] (decly) {i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [llvm, fill = apple!40, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [llvm, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [llvm, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\hfill
\resizebox{0.49\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm, fill = apple!40] (declx) {\_SYM i32 x };
    \node [llvm, fill = apple!40, below = 0.5 cm of declx] (decly) { i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [llvm, fill = apple!40, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [llvm, fill = apple!40, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [llvm, fill = apple!40, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}

During the propagation analysis of \code{y}, we find that \code{y} is an
argument the function \code{bar}, hence we need to also analyze the function
\code{bar} and compute the roots for it. Note that even though the function
\code{bar} in the bottom basic block is called with an abstract argument, we want
the call to \code{bar} in the \code{else} branch to remain explicit.

Another reachable instruction worth mentioning is \code{ret}, since it means that
we will need to change the signature of the function \code{foo}.

After the propagation of $y$ we have not find any new abstract variable (marked
on the right picture), hence the analysis of the function \code{foo} is finished
with the set of roots $\{x, y\}$.
\end{example}
\end{minipage}}

\add{ todo move exaple to the left }

As described in \autoref{ex:vpa}, the value propagation analysis, besides
the finding the reachable instructions, has to step into function calls with
abstract arguments, marking the arguments as roots of the called function.

Since the called function may have some annotated values, we will distinguish
two types of roots: annotation-dependent and argument-dependent. Let us
illustrate the meaning of this distinction of roots using an example.

\begin{example} \label{ex:roots} Let's have following two functions.
\begin{minted}{cpp}
int bar(int arg) {
    _SYM int y;
    int b = arg;
    if (b < y)
        return b;
    else
        return 42;
}

int foo() {
    _SYM int x;
    int val = 0;
    int res1 = bar(x);
    int res2 = bar(val);
}
\end{minted}
    We can see that variable \code{x} is a root for the function \code{foo} and
    \code{y} is a root for the function \code{bar}. Since \code{bar} is called
    with an abstract argument from \code{foo}, we want to add \code{arg} to
    the roots of \code{bar}. But in further analysis, we do not want to consider
    \code{bar} to be always called with an abstract value.

    Hence we distinguish two sets of roots for the \code{bar} function. One, when
    the function is called with a concrete value, which only includes the single root
    $y$. For the case when the argument is abstract, the propagation results
    into $\{y, arg, b\}$. Since $y$ is always present in the set, because it is
    marked directly in the function, we represent the resulting roots as union
    of annotation-dependent roots ($y$) and argument-dependent roots
    ($arg, b$).
\end{example}

As illustrated in \autoref{ex:roots}, the result of value propagation analysis
is, for each function, a set of annotation-dependent roots and sets of
argument-dependent roots.

Additionally, you may have noticed that depending on the
argument, the result of the function \code{bar} can be abstract. To record this
information, we also add the resulting value \code{res1} to annotation-dependent
roots of the function \code{foo}, since the abstract call was made due to
propagation of its annotation roots.

Because we need to precisely determine the return type of each function, we utilize
the \code{UnifyFunctionExitNodes} pass, provided by the \LLVM toolset, to unify all
exit nodes to single one. Hence in the case
when the function \code{bar} returns an abstract value, we need to \emph{lift} the
value \code{42}, since the it is also a possible return value.

Besides 'downwards' propagation (in the direction of control flow), we need
to handle backwards propagation. This case occurs when some abstract value is
stored into a nonabstract output argument (pointer or reference) of the function,
illustrated below.
\begin{minted}{cpp}
void init(int *x) {
    _SYM int v;
    *x = v;
}

int main() {
    int x;
    init(&x);
}

\end{minted}

To handle the backwards propagation, we mark the output argument as a root of the
function, and examine all the locations from where the function is called. From
the call locations we compute the origin of the argument and put it into the
domain that was stored to it (i.e. variable \code{x} is a symbolic root
in the function \code{main}).

Last of all, we need to handle abstraction of aggregate types. Since we enable
only annotation of scalar types, abstract elements in aggregates may occur
only due to stores. The problem to be solved is how to mark domains of a
particular element of the aggregate. For this purpose we introduce a tree-like
structure, that mimics the structure of aggregate type and stores domains of
its scalar elements (shown in \autoref{ex:fieldtrie}).

In \LLVM, when an aggregate is created by an \code{alloca} instruction, we get a
pointer to the aggregate base. In order to access the elements, we need to
\code{load} using the pointer to the aggregate. The specifis element is then
accessed using the \code{getelementptr} instruction, which computes for a given
sequence of indices, an offset into the structure.

\begin{example} \label{ex:fieldtrie}
Consider a nested structure represented in \LLVMIR:
\begin{minted}{llvm}
%Widget = type { i64, %Store* }
%Store = type { i64, i64, i64 }
\end{minted}

We have utilized this indexing strategy to create a tree-like representation
of types. The representation of \code{Widget} as tree is shown on the
left picture. The edges of the tree represent the access operation type
(i.e \code{load} operation or index in \code{getelementptr} instruction)
\cite{LLVM:langref}.

Since we work only with abstraction of scalar types, we need to focus only on
the leafs of the tree structure. To denote a domain we assign the type of the
abstraction to the leaf. Beacause we only need an information about abstract
elements, we do not store the rest of the leafs. Example abstract structure
with multiple domains is on the right picture.

\begin{center}
\begin{minipage}[t]{.47\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm, level
    distance = 1cm}]
    \tikzstyle{fieldnode} = [
        ftreenode,
        minimum width = 0.2cm,
        minimum height = 0.2cm
    ];
    \node [fieldnode, label=180:{\small\code{Widget*}}] {}
        child { node [fieldnode, label=180:{\small\code{Widget}}] {}
            child{ node [fieldnode] {}
                child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                edge from parent node [left, above=1pt] {\small\code{0}} }
                child{ node [fieldnode, label=0:{\small\code{Store*}}] {}
                    child{ node [fieldnode, label=0:{\small\code{Store}}] {}
                        child{ node [fieldnode] {}
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [left, above=1pt] {\small\code{0}} }
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [left] {\small\code{1}} }
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [right, above=1pt] {\small\code{2}} }
                        edge from parent node [left] {\small\code{0}}
                        }
                    edge from parent node [left] {\small\code{load}}
                    }
                edge from parent node [right, above = 1pt] {\small\code{1}}
                }
            edge from parent node [left] {\small\code{0}}
            }
        edge from parent node [left] {\small\code{load}}
        }
    ;
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{.47\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm, level
    distance = 1cm}]
    \tikzstyle{fieldnode} = [
        ftreenode,
        minimum width = 0.2cm,
        minimum height = 0.2cm
    ];
    \node [fieldnode, label=180:{\small\code{Widget*}}] {}
        child { node [fieldnode, label=180:{\small\code{Widget}}] {}
            child{ node [fieldnode] {}
                child{ node [fieldnode, label=180:{\color{orioles}zero}] {}
                edge from parent node [left, above=1pt] {\small\code{0}} }
                child{ node [fieldnode, label=0:{\small\code{Store*}}] {}
                    child{ node [fieldnode, label=0:{\small\code{Store}}] {}
                        child{ node [fieldnode] {}
                            child{ node [fieldnode, label=0:{\color{orioles}symbolic}] {}
                            edge from parent node [left] {\small\code{1}} }
                        edge from parent node [left] {\small\code{0}}
                        }
                    edge from parent node [left] {\small\code{load}}
                    }
                edge from parent node [right, above = 1pt] {\small\code{1}}
                }
            edge from parent node [left] {\small\code{0}}
            }
        edge from parent node [left] {\small\code{load}}
        }
    ;
\end{tikzpicture}
\end{minipage}
\end{center}
\end{example}

During value propagation analysis, we store these trees for each variable.
To be consistent, we represent scalar types in the same way (i.e. they are
represented as a single node). For pointer types, the trees look like a chain of
\code{load} edges.

\begin{summary}
In summary, the value propagation analysis takes annotated \LLVM bitcode and
returns the roots of abstraction. Roots are those \LLVM values that introduce an
abstract value in the context of a single function. We have divided roots into two
kinds: annotation-dependent, that are dependent directly on annotated values
in the function, and argument-dependent, that are dependent
on the abstracted arguments of the function. At the end of the section we
have shown how to analyze aggregate types with multiple domains using a tree
representation of the structures.
\end{summary}

\subsection{Lifting of instructions}
Having computed the abstraction roots from value propagation analysis, we can
start program transformation. In this \LLVM pass, we want all the instructions
that work with abstract values to be replaced by abstract intrinsics.
Additionally all the required function prototypes have to be created, and
appropriate abstract implementation inserted into them.

From the abstraction roots computed in VPA we can do the transformation per
function. But first of all, the function prototypes are created. This is done
because during the transformation we may need to call some abstract function,
that has not been abstracted yet, but to create a call of function its prototype
is sufficient. This approach also elegantly deals with recursive calls.

\add{ add sidenote about recursive calls }

Function prototypes are created based on the abstraction roots. Basically
the transformation iterates over all the sets of argument-dependent roots and
the prototype is created based on the arguments included in the given set.
Moreover, the return type is deduced from the reachability of the \code{ret} instruction
from the roots.

During this phase and further the transformation pass builds a mapping between
concrete types and abstract types. The abstract types is a \LLVM structure
expressing the abstracted concrete type and the domain. For example an integer
in symbolic domain is represented as follows.

\begin{minted}{llvm}
%lart.sym.i32 = type { i32 }
\end{minted}

Names of abstract types are always prefixed by \code{lart}, followed by the
domain name and finished by a name of a concrete \LLVM type.

In this type system we can create desired prototypes for the functions from our
example program (\autoref{fig:exampleprogram}). Expecting that the function \code{foo}
does not take any abstract value we need only to change to return type of the
signature. For the function \code{bar} there are two cases: one does not take any
abstract value and second one has one abstract argument. After prototype
creation, the resulting \LLVM declarations look like this.
\begin{minted}{llvm}
declare %lart.sym.i32 @foo.2(i32, i32)
declare %lart.sym.i32 @bar.2(i32, %lart.sym.i32)
\end{minted}
Since \LLVM requires distinct names for each function, indices are added in
the order of prototype creation. The transformation is done per function,
specifically for each set of roots separately.

From given roots we compute the reached set of instructions and sequentially
transform them. In order to have all the instructions transformed before their
result is used further in the function, we transform them in a reverse postorder manner.

We will represent abstract operations as function calls, with similar naming
conventions as \LLVM-native intrinsic operations \cite{LLVM:langref}. The
operations still need to keep track of the domain type and the base \LLVM instruction.
For example for the \code{add} operation, we create an abstract equivalent:
\begin{minted}{llvm}
%res = call %lart.sym.i32
            @lart.sym.add.i32(%lart.sym.i32 %a,
                              %lart.sym.i32 %b)
\end{minted}

When an instruction takes both a concrete and an abstract argument, we need to
transform the concrete value into the correct domain. This is done by a \code{lift}
operation, which creates an abstract constant (described in
\autoref{sec:absdom}).

\marginpar{More unpleasant way exists, where combinatorial amount of abstract
operations is generated. For obvious reasons we did not take this path.}

\add{ add sidenote about within domain transfomation }
Besides within domain transformations, we need to solve inter-domain interactions. As
mentioned in \autoref{sec:interactions}, we need to transform in-domain booleans
into tristate manipulations. This has to be done to unify control flow
handling when dealing with multiple domains. Let's consider the following condition
to be abstracted.
\begin{minted}{llvm}
%cond = icmp sgt i32 %a, %b
br i1 %cond, label %then, label %else
\end{minted}
Since we do not know without further analysis whether the resulting value of
\code{icmp} is used in control flow determination, we prefer the result to
remain in the abstract domain. Only when we found a use of abstract boolean in
some control flow instruction (i.e. branching), we inserts conversion operation
from abstract boolean to tristate (described in \autoref{sec:interactions}).
Subsequently we lower the tristate into concrete boolean in order to preserve
explicit control flow:
\marginpar{During the execution, the lowering of tristate may call a non
deterministic choice if its value is uncertain i.e.~\emph{maybe}.}
\begin{minted}{llvm}
%cond = call %lart.sym.i1
             @lart.sym.icmp_sgt.i32(%lart.sym.i32 %a,
                                    %lart.sym.i32 %b)
%tris = call %lart.tristate
             @lart.sym.bool_to_tristate(%lart.sym.i1 %cond)

%bool = call i1 @lart.tristate.lower(%lart.tristate %tris)
br i1 %bool, label %then, label %else
\end{minted}

\begin{figure}
\begin{example}
With created prototypes the whole transformation of our example program from
\autoref{fig:exampleprogram}:

\bigskip
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {i32\textsubscript{sym} \textoverline{x}};
    \node [llvm, below = 0.2 cm of declx] (decly) {i32\textsubscript{sym}
    \textoverline{y}};
    \node [llvm, below = 0.2 cm of decly] (lifta) {\textoverline{a} = lift\textsubscript{sym}(a)};
    \node [llvm, below = 0.2 cm of lifta] (cond) { \textoverline{cond} = \textoverline{x} <\textsubscript{sym}
    \textoverline{a} };
    \node [br, below = 0.5 cm of cond] (br) { lower(\textoverline{cond}) };
    \node [llvm, below left = 0.5 cm of br] (lift4) { \textoverline{4} =
    lift\textsubscript{sym}(4) };
    \node [llvm, below = 0.2 cm of lift4] (brtrue) { \textoverline{y} =
    \textoverline{x} +\textsubscript{sym} \textoverline{4} };
    \node [llvm, below right = 0.5 cm of br] (liftbar) { y = bar(a, b) };
    \node [llvm, below = 0.2 cm of liftbar] (brfalse) { \textoverline{y} =
    lift\textsubscript{sym}(y) };
    \node [llvm, below = 2.5 cm of br] (retcall) { \textoverline{y} =
    \textoverline{bar}(a, \textoverline{y}) };
    \node [llvm, below = 0.2 cm of retcall] (ret) { ret \textoverline{y} };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly) (lifta) (cond)] (bb1) {};
        \node [bb, fit = (brtrue) (lift4)] (bb2) {};
        \node [bb, fit = (brfalse) (liftbar)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32\textsubscript{sym} foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (cond) -- (br);
    \draw [flow, very thick] (br.west) -| (lift4.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (liftbar.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\bigskip

In the picture, symbolic values are represented by bar over them.
A notable change in the program are points where concrete values have to be
lifted to symbolic domain. Besides that a transformation of conditional
branching occur, with explicit branch over the lowered tristate. Lastly the
correct symbolic form of \code{bar} function has to be called. We can observe that call
with concrete arguments remains untouched.
\end{example}
\end{figure}

When dealing with aggregate or pointer types, we know that they are not
abstract, so we do not need to create abstract operations with them. Hence the
original instructions manipulating with aggregates remain untouched, except the
instructions that extract or inserts scalars to them.

In those situations, we take the use of results from the value propagation analysis.
Knowing which elements of aggregate are abstract, when the program tries to
reach to some of them (through some pointer) we just simply bitcast
the resulting concrete pointer to pointer to an abstract element. We can afford
to this because we know that in the particular memory location, an abstract value is
stored.

However this approach has its limitations, since the abstract representation has
to fit into the size of original type. As consequence we are not currently able
to store a pointer to representation of abstract type into smaller types then
pointer type (i.e. \code{i64} on 64-bit architecture)\marginpar{In \DIVINE is a
preparation in process, to be able to store an additional data into 'shadow'
memory. With it, \DIVINE will be able to maintain a pointer in the smaller type.}.

But as a big advantage of this approach is that we do not need to create an
abstract aggregate types and handcraft the whole structures. This way we also
can preserve the pointer arithmetic operations, since the offsets to the
aggregates do not change.

\begin{summary}
Given the roots of abstraction, we have shown how to transform program into
abstract intermediate form. Firstly we have constructed prototypes of functions
with abstract signature (either has some abstract argument or returns abstract value).
Further we have transformed all instructions reachable from the abstraction
roots into abstrct intrisics. Accordingly we have introduced representation of
abstract types. During the transformation process we have also dealed with an
interaction with control flow using transformation to tristate and consequent
lowering to \LLVM boolean.
\end{summary}

\subsection{Backwards constraint propagation and value restriction}
\label{sec:bcp}
When a branch is taken due to a lowered tristate, we need to restrict the values
engaged in the condition. Consider that branch is taken, based on comparison $a
> 10$, where $a$ is an abstract value. If we do not have any restriction on $a$
yet and branch was taken nondeterministically, as consequence the value of $a$
has to be greater than 10. However, when the branch was not taken,
the value has to be 10 or less. We will utilize this fact, to restrict the value
right after the branch was taken.

We can compute the value restrictions independently of the abstract domain, as
long as the abstract domain provides a right set of primitives. Concretely we
need to be able to infer a restricted value from a given value and some predicate
instruction on which we want to base the restriction, along with an actual
result of the predicate evaluation. For this purpose we have established
an \code{assume} operation:
\begin{minted}{llvm}
%x = call %lart.sym.i32
          @lart.sym.assume(%lart.sym.i32 %a,
                           %lart.tristate %cond,
                           i1 true)
\end{minted}
Meaning that value \code{\%x} carries the restricted value of \code{\%a}
according to the satisfied condition \code{\%cond}.

The insertion of assumes in two steps, first of all we restrict only the
tristates that are directly lowered and used in unconditional branching. By this
transformation we simply mark which path is taken based on the nondeterministic
choice.

Further we proceed in restriction of variables that are engaged in the condition
predicate. It may seem reasonable to carry on backwards in the program history to
provide more precise restriction, but backwards constraint propagation has some
limitations.

\add{ constrain propagation example }

Since the whole computation has to be done statically it is not clear how far the
backwards look up should proceed. The other limitation is that, the propagation
can only sufficiently analyze only the local static scope, and cannot infer
information from function arguments and global state.

Consequently we need to make trade-off between work given to domain
implementation and simple constrain analysis. Since the symbolic domain is
capable to acquire the implications resulting from simple restriction over the
branch condition we have decided to retain only simple assume insertion
algorithm for this thesis.

The symbolic domain can simply collect the restrictions as path condition and
restricts the possible variable evaluations based on the whole path
(in detail description of utilization of assumes in symbolic domain will be
described in \autoref{sec:symbolic}).

In addition to backwards analysis, assumes insertion demands a few minor
modifications to bitcode. When control flow of restricted value merges with
control flow from another restriction or original definition, we need to insert
appropriate $\varphi$ nodes.

For simpler dependency analysis and $\varphi$ nodes insertion, we have decided
to insert the restrictions on the edge between restricting branch and consequent
basic block. The insertion is implemented as creation of a new basic block
between two blocks redirection of the branch (illustrated in the
\autoref{ex:assumes}).

\begin{figure}
\begin{example} \label{ex:assumes}
Analyzing the example program, we need to restrict values depending on one
conditional branching.
\autoref{fig:exampleprogram}:

\bigskip
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {i32\textsubscript{sym} \textoverline{x}};
    \node [llvm, below = 0.2 cm of declx] (decly) {i32\textsubscript{sym}
    \textoverline{y}};
    \node [llvm, below = 0.2 cm of decly] (lifta) {\textoverline{a} = lift\textsubscript{sym}(a)};
    \node [llvm, below = 0.2 cm of lifta] (cond) { \textoverline{cond} =
    \textoverline{x} <\textsubscript{sym} \textoverline{a} };
    \node [br, below = 0.5 cm of cond] (br) { lower(\textoverline{cond}) };
    \node [llvm, fill=apple!40, below left = 0.5 cm of br] (asstrue)
    {\textoverline{x} = assume(\textoverline{x} <\textsubscript{sym} \textoverline{a})};
    \node [llvm, below = 0.5 cm of asstrue] (lift4) { \textoverline{4} =
    lift\textsubscript{sym}(4) };
    \node [llvm, below = 0.2 cm of lift4] (brtrue) { \textoverline{y} =
    \textoverline{x} +\textsubscript{sym} \textoverline{4} };
    \node [llvm, fill=apple!40, below right = 0.5 cm of br] (assfalse)
    {\textoverline{x} = assume(\textoverline{x} >=\textsubscript{sym}
    \textoverline{a})};
    \node [llvm, below = 0.5 cm of assfalse] (liftbar) { y = bar(a, b) };
    \node [llvm, below = 0.2 cm of liftbar] (brfalse) { \textoverline{y} =
    lift\textsubscript{sym}(y) };
    \node [llvm, below = 4 cm of br] (retcall) { \textoverline{y} =
    \textoverline{bar}(a, \textoverline{y}) };
    \node [llvm, below = 0.2 cm of retcall] (ret) { ret \textoverline{y} };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly) (lifta) (cond)] (bb1) {};
        \node [bb, fit = (brtrue) (lift4)] (bb2) {};
        \node [bb, fit = (brfalse) (liftbar)] (bb3) {};
        \node [bb, fit = (asstrue)] (bbt) {};
        \node [bb, fit = (assfalse)] (bbf) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32\textsubscript{sym} foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (cond) -- (br);
    \draw [flow, very thick] (asstrue) -- (lift4);
    \draw [flow, very thick] (assfalse) -- (liftbar);
    \draw [flow, very thick] (br.west) -| (asstrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (assfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\bigskip

In this simple case, proper assumes are inserted on the edges between branching
and consequent basic blocks. Remark that separate basic blocks were created for
the assumes. Creation of new basic blocks does not make any diffence in this
program, but for more complicated functions with loops edge splitting needs
to be done in order to enable $\varphi$ nodes creation.

We may see, that assumes return a new value for restricted variable, that is
used in following code. After an inspection we may notice that assume in
\emph{false} branch is unnecessery since the value of restricted
\code{\textoverline{x}} never used afterwards, but we don't do any optimizations
in this case.
\end{example}
\end{figure}

\begin{summary}
When abstract program takes nondeterministic conditional branching, it needs to
constrain abstract values that appeard in the condition of branching. For this
purpose we have introduced \code{assume} calls. Given abstract value, condition
predicate, and result of the predicate, \code{assume} call computes the
constrained value. In the section we have also introduced backwards constraint
propagation, that provides more precise restrictions.
\end{summary}

\subsection{Domain manipulations insertion}

After all the analysis and transformations are finished, we can proceed to actual
domain insertion. Since we now know all the instructions that have to be
transformed with relevant information about the domain, we do the substitution
per instruction without any further analysis.

Similarly as in the abstraction of instructions from the second phase we proceed in
the order, to have all the operations substituted before they are
used further in the bitcode (reverse postorder). When processing an
instruction, we look on the domain tag (in the name of the abstract intrinsic)
and accordingly choose the appropriate implementation. In current setting the
abstract intrinsics are replaced by calls to functions implementing the domain
transformers. This may optimized in the future as inlining of block of code or
direct replacing by single instruction (possible for simpler domain operations),
to minimize the number of call the bitcode.

From the domain implementation point of view, there are two things to be
provided. First, the implementation of transformers over the domain, including
the special ones --- \code{assume}, \code{lift} and \code{bool\_to\_tristate}.
And secondly, the implementation of lowering function, that given an abstract
\LART intrinsic and its arguments, produces a corresponding call to domain
transformer.

The limitation of whole transformation is that the abstract values must have
run-time representation using scalar values from concrete domain. This is not a
problem for simpler domains like $Z$, but for symbolic abstraction the value
representation has to be tweaked. As proposed in \autoref{sec:sym}, the data
have to be represented by tree-like structure. This representation can be then
stored behind a pointer, and stored on program heap. But by this approach we
introduce memory leaks to transformed bitcode. The other way is to copy symbolic
representation on every taken operation, what is quite inefficient in actual
bitcode runtime. Due to this we have used a special pointers provided by
\DIVINE (weak pointer), which \DIVINE recognize as non leaking. These pointers
are then maintained by \DIVM in similar fashion as garbage collectors do.
Hence during the verification \DIVINE will distinguish the pointers to symbolic
values and conceal the memory leak.

\begin{summary}
In this section we have shown, how the actual domain is inserted into the
program. Insertion is done in per instruction manner, where given an abstract
instruction (intermediate intrinsic call) we replace it by corresponding code
that implements manipulation in given domain.
\end{summary}

\section{Symbolic state space exploration} \label{sec:symbolic}
When the program transformation is finished, abstracted program is given to
\DIVINE for model checking phase. Since the equality and reachability check
of the symbolic states cannot be done in only explicit manner, we need to employ
a similar exploration algorithm as \SymDIVINE (see \autoref{sub:symdivine}).

As described in previous sections, symbolic data in abstracted program are
described by data definitions (that are created by symbolic operations) and
path condition (that is defined by assumes taken during the program execution).
Then during the interpretation we need to check whether the state is
reachable, this is done by query on \SMT solver, whether the already taken
path condition is satisfiable. And secondly we need to distinguish whether we
have already seen the reachable state, that is done via equality check. Now we will
describe both queries in higher detail.

\subsection{Exploration algorithm}
The symbolic exploration algorithm, similarly as explicit algorithm, uses \DIVM
to interpret the \LLVM instructions. During the interpretation, abstracted
operations manipulates with symbolic values, according to the inserted implementation of
symbolic domain (i.e.~they operates on formula trees, see \autoref{sec:sym}).
Note that \DIVM has no clue about the symbolic representation and interprets
program as ordinary \LLVM bitcode.

The data definitions are currently represented as global data structure, that
keeps all the references to the formula trees. This structure has to be global
for abstract operations to be able to access it.

In order to create path condition, program needs to track constraints over the
symbolic values. This is achieved with \code{assume} calls inserted after the
control flow branching. Let's have a program with some branches:
\begin{minted}[linenos]{cpp}
__SYM int x;
if (x < 10) {
    if (x > 5) {
        ...
    }
}
\end{minted}
In the program above on the 4.~line we need to ensure that value of \code{x} is
between 5 and 10. This has to be done because the comparison on symbolic values
does not restrict the original value, only creates a nondeterministic choice
whether the branch is taken or not, hence value of \code{x} is arbitrary even if
we have decided that it is smaller then 10 in the first condition.

The restriction is achieved, right away after the branch is taken, by
\code{assume} call. To maintain the restrictions a path condition is stored in
some global data structure. Then, when the \code{assume} call is executed a
restriction is appended to the path condition. This is done in the
implementation of the \code{assume} call in the symbolic domain
(i.e.~manipulation with path codition is interpreted by \DIVM). Looking back to
the example, the path condition on the 4.~line is represented by two
restrictions: $(x < 10)$ and $(x > 5)$.

When a new state is generated we need to check whether the state is reachable,
this is done similarly as in \SymDIVINE, by checking the satisfiability of the
path condition. The path condition for \SMT query is created as conjunction of
all restrictions taken so far, i.e.~for line 4, the path condition looks like:
$(x < 10) \wedge (x > 5)$.

Since the path condition is changed only when the \code{assume} call is
interpreted, it is not necessary to check reachability for every state, but only
for those when the path condition changes, i.e.~after the \code{assume} call.

\subsection{Equality check}

Similarly as in \SymDIVINE (see \autoref{subsec:equality}), we will firstly
compare the explicit part of the states and then the symbolic part using the
\SMT solver.

Let's have two states $s_1$ and $s_2$, in the same control flow location, with
the same memory shape. This is when $s_1 = (c, m, \varphi)$ and $s_2 = (c,
m, \psi)$ for control location $c$, memory shape $m$ and path conditions $\varphi$ and $\psi$.

For example representation of the memory configuration of compared states (in
the same control location) may look like this:

\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.3cm, level
    distance = 1cm}]

    \tikzset{state/.style = {draw, inner sep = 5pt,rectangle split, rectangle split horizontal,
    rectangle split parts=6}, minimum height = 0.8cm}
    \node [state, label={$s_1$}] (s1)
        {$c_1$
         \nodepart{two} \dots
         \nodepart{three} $p_1$
         \nodepart{four}~~~~\dots~~~~
         \nodepart{five} $p_2$
         \nodepart{six} \dots};
    \node [state, right = 1 cm of s1, label={$s_2$}] (s2)
        {$\widehat{c}_1$
         \nodepart{two} \dots
         \nodepart{three} $\widehat{p}_1$
         \nodepart{four} ~~~~\dots~~~~
         \nodepart{five} $\widehat{p}_2$
         \nodepart{six} \dots};
    \node [below = 0.5 cm of s1.three south, ftreenode] (t1) {$+$}
        child{ node [ftreenode] {$v_1$} }
        child{ node [ftreenode] {$1$} }
    ;

    \node [below = 0.5 cm of s1.five south, ftreenode] (t2) {$*$}
        child{ node [ftreenode] {$v_2$} }
        child{ node [ftreenode] {$v_3$} }
    ;

    \node [below = 0.5 cm of s2.three south, ftreenode] (t3) {$+$}
        child{ node [ftreenode] {$+$}
            child { node [ftreenode] {$v_1$} }
            child { node [ftreenode] {$1$} }
        }
        child{ node [ftreenode] {$1$} }
    ;

    \node [below = 0.5 cm of s2.five south, ftreenode] (t4) {$*$}
        child{ node [ftreenode] {$v_2$} }
        child{ node [ftreenode] {$v_3$} }
    ;

    \draw [->] (s1.three south) -- (t1);
    \draw [->] (s1.five south) -- (t2);
    \draw [->] (s2.three south) -- (t3);
    \draw [->] (s2.five south) -- (t4);
\end{tikzpicture}
}
\end{center}

States in the same control location, do not differ in the memory shape. We can
see that concrete values ($c_1$, $\widehat{c}_1$) and symbolic values
($p_1, p_2, \widehat{p}_1, \widehat{p}_2$) occupy the same memory locations in
both states. Symbolic values are represented by formula trees, as described in
\autoref{sec:sym}. In the leafs of trees we maintain constants or symbolic
variables. We kindly remark that, even though symbolic values are represented by
tree, we do not consider them in the memory shape comparison. Only the position
of symbolic values $p_k$ is considered in the comparison.

We will denote symbolic variables of both states as $v_1, \dots, v_n$. The
values $p_1, \dots, p_k$ in the state $s_1$ and $\widehat{p}_1, \dots,
\widehat{p}_k$ in state $s_2$ are results of transformers over the symbolic
values. From the logic point we consider them as nullary functions, these
functions return predicates over the symbolic values. For example $p_1$ defines
a nullary function $v_1 + 1$.

Comparing these states, we compare the explicit values directly (i.e.~$c_i =
\widehat{c}_i$ for all the explicit values in the states).  When explicit parts
are same we need to compare the symbolic parts. To decide whether the sets of
concrete states represented by $s_1$ and $s_2$ are equal, we define a formula
$equals(s_1, s_2)$:
\begin{equation*}
\begin{aligned}
    pc_{eq}(\varphi, \psi) \defeq & ~(\varphi \iff \psi) \\
    val_{eq}(s_1, s_2) \defeq & ~\varphi \implies (p_1 = \widehat{p}_1 \wedge \dots \wedge p_k = \widehat{p}_k ) \\
    equals(s_1, s_2) \defeq & ~\forall~v_1,\dots, v_n (pc_{eq}(\varphi, \psi)
    \wedge val_{eq}(s_1, s_2))
\end{aligned}
\end{equation*}
The formula says, that states are equal when the path conditions are
equivalent (defined by $pc_{eq}$) and data definitions describe the same set
of data valuations (defined by $val_{eq}$). We check for data equality only in
the case when path conditions are satisfiable. Since the path conditions must
be equivalent, it does not matter which one we check for satisfiability.

\marginpar{In reality when we check for equality of two states, we just need to
find proof of that there exists an evaluation that distinguishes both states.
This is done easily by querying the \SMT solver for $\neg equals(s_1, s_2)$.
Hence we have simplified the work for solver, because it has to find a proof only
for existential quantifier used in negated formula instead of universal from
original formula.}
Equality of data valuations is defined in per value manner. We couple the
corresponding symbolic values defined by nullary functions and compare them for
equality. These equalities have to be satisfied for all free variables (symbolic
inputs).

From the example, we investigate whether $p_1 = \widehat{p}_1 \wedge p_2 =
\widehat{p}_2$ for all possible values of symbolic variables. Expanding nullary
functions we get $(v_1 + 1) = ((v_1 + 1) + 1) \wedge (v_2 * v_3) = (v_2 * v_3)$. Hence
we ask the \SMT solver whether this formula is satisfiable for all possible
values of $v_1, v_2, v_3$.

Hence to check whether \DIVINE encounters a new state $s$, we query the \SMT solver
for equality with all already reached states in the same control location with the same memory shape.

\add{ comparison to symdivine }

\add{ add chapter summary }


