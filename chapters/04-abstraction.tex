\chapter{Program abstractions}\label{ch:abstraction}

As we have already seen, abstractions are applicable in many ways in model
checking and software verification in general. In this chapter, we propose a novel
approach for verification of programs with inputs. We aim to provide a tool for
abstraction of inputs with only minor modifications to the model checker.

One of the approaches to handle inputs is the symbolic model checking (described
in \autoref{ch:divine}). We may consider the symbolic representation as a form
of abstraction, since input data are abstracted into the sets of values. In this
chapter, we review how programs are abstracted more generally. First, we
introduce a simpler abstract domain, which does not require that much technicalities as the symbolic abstraction. Thereafter, we present an
approach to abstractions via program transformation. At the end of the chapter,
we explore how the transformed program interacts with \DIVINE.

\section{Approaches to abstraction}

In general, there are two main approaches to do abstraction of programs. A
traditional way of abstraction engines is to interpret the given
program~\cite{Cousot79}. During interpretation, the abstraction tool needs to
interpret code according to some semantics (either encoded in the tool or
given). A similar approach is used in \SymDIVINE. During the interpretation of
the \LLVM bitcode, \SymDIVINE manipulates a formula representing a multi state,
as described in \autoref{sub:symdivine}.

Another alternative to perform an abstraction is to insert the abstraction
directly into the program. This can be achieved with a source-to-source
transformation of the program. After the transformation, the program will
manipulate with abstract values instead of concrete ones. For example, in the
case of symbolic abstraction, the abstracted program manipulates formulae that
represent input values, instead of concrete values. The abstracted program after
the transformation behaves almost like a traditional program. The main difference
is that the abstracted program may become nondeterministic. This can happen if
a branching in the program depends on an abstract value and both directions of
the branch are possible.

When comparing the above-mentioned approaches, each of them comes with its pros
and cons. The transformation approach does not complicate the verification tool
since it is performed by the stand-alone tool before the verification process
starts. As a consequence, it minimizes the probability of creating a bug in the
verification tool. Further, we can benefit from the verification of the
transformed program in the way that it will also check the correctness of the
inserted abstraction, since it is encoded into the verified program. This is not possible
in interpretation approach, because the abstraction is part of the interpreter.

On the other hand, the tool that performs a transformation requires a more
complicated design, since most of the computation has to be done statically (we
can not benefit from the values computed during a runtime as interpreters). Also, the verification of the transformed program may potentially become slower, since
abstract operations have to be interpreted by the verification tool.  Both
approaches enable pluggable abstract domains, by generalising their common
analyses, as will be shown in this chapter.

Since \DIVINE is a powerful explicit-state model checker, we have decided not to
complicate the precisely crafted interpreter (\DIVM), and take the second
approach. Besides having a symbolic abstraction as the primary goal, we have
realized that the core of transformation is usable for arbitrary abstraction, as
will be shown in this chapter.

\section{Abstract domains} \label{sec:absdom}

When dealing with an abstraction of programs, first of all, one has to define an
abstract domain for program variables and semantics of operations on them. From
a model checking point of view, we want to define a transformation from
concrete states to abstract states, as described in \autoref{subsec:amc}. To
formulate the transformation, we need to specify how concrete types are mapped to
abstract types, similarly with values and operations on then.

\newpage

\hrule

\bigskip
\noindent
\textit{An abstract domain is an abstract algebra, implemented as a library module,
providing a description of abstract program properties and abstract property
transformers describing the operational effect of program instructions and
commands in the abstract program. Abstract domains are often complete lattices.
--- P. Cousot \cite{Cousot79} }
\bigskip

\hrule

\bigskip

In order to define the transformation from a concrete domain to an abstract
domain, we have to describe the concrete domain first. Values of the concrete
domain for \LLVM are partitioned according to their types. For simplicity, we
work only with a subset of \LLVM types -- integer types (scalar types), pointer
types and aggregate types. Moreover, we do not permit abstraction of pointers,
since pointer analysis is beyond the scope of this thesis, but we permit
concrete pointers to abstract values.
\begin{definition}
A set of concrete scalar types $S$ is a set of all integer types $S = \{
s_i \mid i \in \mathbb{N}^+ \}$, where $i$ is the bitwidth of the integer type
$s_i$.
\end{definition}
From the scalar types, we want to build all possible pointer types and
aggregate types. We denote a pointer to type $t$ as $t*$, and an aggregate type
consisting of types $t_1, \dots, t_n$ as a tuple~$(t_1, \dots, t_n)$.

\begin{definition}
    For each set of scalar types $T$, we define a set of types $\Gamma_T$
    inductively as follows:
    \begin{itemize}
        \item $T \subseteq \Gamma_T$, i.e.~each scalar type is contained in the
            $\Gamma_T$,
        \item if types $t_1, \dots, t_n \in \Gamma_S$ then
            $(t_1, \dots, t_n) \in \Gamma_T$, where $n \in \mathbb{N}^+$,
        \item if $t \in \Gamma_T$ then $t* \in \Gamma_S$.
    \end{itemize}
\end{definition}
Note, that a $\void$ type is not in the set of created types because \LLVM does
not permit a pointer to $\void$ or an aggregate to contain a $\void$. Hence we
will add the $\void$ type as an extra type in the domain definition. With a
created type system we define a concrete domain, which assigns values to
concrete types and defines a set of possible operations over them. A set of
values in concrete domain consists of \LLVM values that correspond to the
restricted type system.
\begin{definition}
    A concrete domain $C$ is a tuple $(T_C, V_C, I_C, O_C)$, where:
    \begin{itemize}
        \item $T_C = \Gamma_S \cup \{ \void \}$ is a set of concrete types,
        \item $V_C$ is a set of all possible \LLVM values,
        \item $I_C \colon T_C \to \mathcal{P}(V_C)$ assigns to a type a set of
            all values, of that type,
        \item $O_C$ is a set of \LLVM operations, where each operation has
            input types and an output type.\sidenote{\LLVM operations are not
            functions, because they can have side effects.}
    \end{itemize}
\end{definition}

To set up an abstraction of \LLVM, we define an abstract domain~$A$. First, we
define an abstract type system $T_A$. Besides all abstract types, the abstract type
system needs to define aggregates that may contain both abstract and concrete
values. Moreover, we require that for each concrete scalar there exists an abstract scalar.
We call a bijection between concrete scalars and abstract scalars $\alpha$. The
function $\alpha$ can be naturally extended to all abstract types as $\overline{\alpha}$.

In addition to the concrete domain, an abstract domain has to describe how concrete
types are mapped to abstract types, concrete values to abstract values and
similarly concrete operations to abstract operations:
\begin{definition}
    An abstract domain $A$ is defined as a tuple
    \[(\alpha, T_A, V_A, I_A, O_A, (\lift_s)_{s \in S}, (\low_s)_{s \in
    S}, \lift_O),\] where:
\begin{itemize}
    \item $\alpha$ is a bijection between concrete and abstract scalars,
    \item $T_A = \Gamma_{S \cup \alpha( S )} \cup \{\void\}$ is a set of all abstract types,
        which is generated from concrete and abstract scalars,
    \item $V_A$ is a set of all possible abstract values,
    \item $I_A \colon T_A \to \mathcal{P}(V_A)$ assigns to a type a set of
        abstract values, of that type; we require that interpretation of
        concrete types remains the same, i.e.~$\forall t \in T_C . I_A( t ) =
        I_C( t )$,
    \item $O_A$ is a set of abstract operations,
    \item $\lift_s \colon I_C( s ) \to I_A( \alpha( s ) )$ is a function
        for scalar value abstraction of concrete type $s \in S$,
    \item $\low_s \colon I_A( \alpha( s ) ) \to \mathcal{P}( I_C( s ) )$ defines a
        transformation from an abstract scalar value to a corresponding set of concrete values,
    \item $\lift_O \colon O_C \to O_A$ is a transformation of
        concrete operations to abstract operations.\sidenote{$\lift_O$ has to
        preserve a type correspondence of operations. When instruction returns
        type $t$, the abstracted instruction has to return type
        $\overline{\alpha}(t)$. Similarly with operation arguments.}
\end{itemize}
\end{definition}

As an example of a simple domain, we define a three-valued domain $Z$.
This domain simplifies all integer values to only three possible values:
\[ V_Z = \{ \textit{zero}, \textit{nonzero}, \textit{unknown} \}.\]
The values represent whether the abstracted variable is zero, non-zero or that
we do not know anything about the value of the variable (\emph{unknown}).
Hence, to all abstract integer types correspond only those three possible
values. The definition of $\lift_s$ and $\low_s$ in the domain $Z$ is straightforward:
\[
  \lift_s(v) =
  \begin{cases}
    \textit{zero},    & \text{if } 0 = v, \\
    \textit{nonzero}, & \text{if } 0 \not = v.\\
  \end{cases}
\]
The function $lower_s$ is defined in similar way. When we lower an
abstract value, the result is a set that represents all values that can be
represented by the abstract value. The resulting values depends also on the
bitwidth $bw$ of the scalar type $s$:
\[
  \low_s(a) =
  \begin{cases}
        \{0\},                            & \text{if } a = \textit{zero},\\
        \big\{1, \dots ,2^{bw - 1}\big\}, & \text{if } a = \textit{nonzero},\\
        \big\{0, \dots ,2^{bw - 1}\big\}, & \text{else}. \\
  \end{cases}
\]

The abstract operations $O_A$ correspond to \LLVM instructions, but they
manipulate values from $V_Z$. We require that abstract operations preserve the
type correspondence. For example, an abstract instruction $\mathit{add}_{i32}$ is
abstracted \LLVM instruction that adds two 32-bit integers and returns 32-bit
result. The type signature of the abstract operation has to correspond with the
types in the domain $Z$: ${i32}_Z \times {i32}_Z \to {i32}_Z$. 
We define a semantic of operation as:
\[
  \mathit{add}_{i32}(a, b) =
  \begin{cases}
    a,      & \text{if } b = \textit{zero},\\
    b,      & \text{if } a = \textit{zero},\\
    \textit{unknown},   & \text{else}. \\
  \end{cases}
\]

Our main goal is to allow the domain to be implemented in some common higher-level
language and compiled into the verified program. For this purpose,
we have chosen language \Cpp{}. To represent the domain $Z$, we have created a
structure that holds a value and a bitwidth of an abstract value:
\begin{minted}{cpp}
struct Zero {
    enum Domain { ZeroValue, NonzeroValue, Unknown }
    int bitwidth;
    Domain value;
};
\end{minted}
A domain defined in this way can be compiled and linked with a verified
program. In compiled code, abstraction leverages the existing type system of
\LLVM, since the compiled definition of the abstraction is encoded by \LLVM scalars.

To cover all \LLVM instructions that can manipulate abstract values, we need to
implement all binary arithmetic operations, bitwise operations, memory
accesses, conversion operations and comparison instructions.

Implementation of abstract instructions may be delivered as \Cpp{} functions over
the corresponding abstract types. For example, an abstract equivalent of the
\code{add} instruction in the domain $Z$ is implemented as
follows.\sidenote{Complete implementation of the domain $Z$ can be found in thesis
attachments, with only minor technical differences. Code in the text is cleaned
for readability.}

\begin{minted}{cpp}
Zero __abstract_zero_add( Zero a, Zero b ) {
    if ( b.value == Zero::Domain::ZeroValue )
        return a;
    if ( a.value == Zero::Domain::ZeroValue )
        return b;
    return Zero::Domain::Unknown;
}
\end{minted}

Likewise, we define all binary and unary operations in the given domain. Notice
that if the operation takes both an abstract and a concrete value (for example an
addition of a constant to the abstract value) we need to bring both values to the same
domain. To do so, we lift the concrete value and perform the operation in the
abstract domain. Additionally, we define memory operations \code{load},
\code{store} and \code{alloca}. Their prototypes and implementation can be found
in attachments of the thesis.

\subsection{Domain interactions} \label{sec:interactions}
In cases when multiple domains interact, special care needs to be
taken. We need to solve two major types of interactions.
We have already seen an interaction between abstract domain and concrete in the form of a $\lift$ and a $\low$. This interaction occurs when a concrete value
has engaged in the abstract instruction. Hence it has to be lifted to the
abstract domain.

Another interaction between domains occurs when control flow depends on an
abstract value (i.e.~in a conditional branch).  But in order to maintain
explicit control flow, the branching instruction needs to take an explicit
value. Hence the abstract Boolean, on which the branching depends, needs to be
lowered to the concrete domain. This can be easily done when we know whether the
value is \emph{true} or \emph{false}. But in the case when the value is
\emph{unknown}, we need to do a nondeterministic choice and proceed in both
possible directions.

To unify handling of interaction with control flow, we introduce a tristate
domain (\emph{true, false, maybe}), and require all abstract domains to define
an operation that takes their abstract Boolean and transforms it to the tristate
domain. We will refer to it as \code{bool\_to\_tristate}. Hence before each
branching on an abstract value, we need to transform it to a generalised form of
tristate and according to the value of the tristate decide whether to do the
nondeterministic choice.\sidenote{A nondeterministic choice is made only when the
tristate has a value \emph{maybe}.}

\subsection{Control flow of abstract programs} \label{subsec:cf}
When a branch is taken due to a \emph{maybe} result of some Boolean expression,
the effect of the branch can be, in addition to its usual concrete semantics,
to restrict the values of some abstracted variables. Abstract interpreters
naturally take advantage of this fact, by building up a path condition
\cite{Rockai15}.

In a transformation-based approach, we can take advantage of the same principle, by
restricting the values on which the branch has been taken. For this purpose, we
require an abstract domain to provide an operation \code{assume}. The operation
\code{assume} has to be able to compute a new value for the constrained
variable from a given \LLVM predicate, such as \code{icmp}, and the actual result of
that predicate (i.e.~\emph{true} or \emph{false}). Control flow of abstract
programs is further examined in \autoref{sec:bcp}.

\subsection{Symbolic domain}\label{sec:sym}

In comparison to the simple domain $Z$, the symbolic domain brings a few
complications. First of all, we need to create a representation of symbolic
data, and implement required operations. In the \SymDIVINE algorithm, the data were
described by two parts: \emph{data definitions} and \emph{path condition} (see
\autoref{sub:symdivine}). We would like to bring this notion of separation of
data and path condition also to the abstracted program.

In \SymDIVINE, data definitions were created by the instructions. In the abstracted program, we will do the same; i.e.~the symbolic definitions will be created either via symbolic
\code{alloca} or as a result of symbolic instruction.

The path condition should reflect the already taken path in the program, hence
it will be described by assumes that were encountered during the interpretation
(i.e.~constraints given by branching).

Symbolic data needs to represent a bit-vector formula, which defines their data.
A typical formula representation is formula tree. For example in the following program, the data representation at the end of the program by formula trees will
be as on the left picture.

\bigskip
\noindent
\adjustbox{valign=t}{
\begin{minipage}{.4\textwidth}
\begin{minted}{cpp}
int x = input();
int y = input();
int a = x + y;
int b = a * x;
\end{minted}
\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}{.55\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm/#1, level
    distance = 1cm}]
    \node [fvar] (x) {$x$};
    \node [fvar, right = of x] (y) {$y$};
    \node [fvar, right = of y] (a) {$a$};
    \node [fvar, right = of a] (b) {$b$};
    \node [fop, below = 0.6cm of a] (plus) {$+$};
    \node [fop, below = 0.6cm of b] (times) {$*$};
    \node [fatom, below = 1cm of x] (xa) {};
    \node [fatom, below = 1cm of y] (ya) {};

    \draw [flow] (x) -- (xa);
    \draw [flow] (y) -- (ya);
    \draw [flow] (a) -- (plus);
    \draw [flow] (b) -- (times);
    \draw [flow] (plus) to [out=-135, in=-45] (xa);
    \draw [flow] (plus) to [out=-135, in=-45] (ya);
    \draw [flow] (times) to [out=-135, in= -45] (plus);
    \draw [flow] (times) to [out=-135, in= -45] (xa);
\end{tikzpicture}
\end{minipage}}

The trees are induced from the \SSA form of the program -- see that each
variable points to the root of its formula tree. The input values $x$ and $y$
point to the nodes representing atoms the formula; i.e.~arbitrary values.
However, $a$ and $b$ point to nodes that represent the abstract values.
Overall, the operations in the symbolic domain are defined as operations on
the tree representation.

\begin{summary}
In this section, we have defined what is an abstract domain in the context of program
transformations. We have established a representation of the abstract domain in
the high-level programing language, accompanied by abstract operations. In
addition to abstract \LLVM instructions we have introduced \code{lift} and
\code{lower} for conversion between the concrete and the abstract domain. We have
examined interactions between domains, as casting between domains and
transformation to the tristate domain. Consequently, we have analyzed the control flow
of an abstracted program, that introduces nondeterministic choices into the
branching. Lastly, we have settled a representation of the symbolic domain via formula
trees.
\end{summary}

\section{Abstraction via program transformation}

In following sections, we will describe the actual implementation of domain
insertion into the program. By providing a universal set of analyses, we aim for
much easier implementation of new abstract domains. Furthermore, we want to
create the transformation in such a manner, that the resulting program will be a
runnable binary, provided the nondeterministic choice implementation.\sidenote{To
implement a nondeterministic choice for a standalone program one can use a standard random
function or a bit-vector of choices} Hence the abstracted program should be
analyzable by arbitrary tools, not just by \DIVINE. Moreover to preserve
the soundness of further analysis we have to define the transformation in an
information-preserving manner.

Ergo, we have chosen the transformation to be done on the \LLVM bit-code level to
mimic the logic of the original program as precisely as possible. The whole
transformation is done in successive standalone \LLVM passes, producing the
abstracted \LLVM bitcode. It is further described in following sections, and put
together in \autoref{fig:transformation}.

\begin{figure}[!ht]
\checkoddpage
\edef\side{\ifoddpage l\else r\fi}
\makebox[\textwidth][\side]{
\begin{minipage}[bt]{\fullwidth}
\prule
\bigskip

\resizebox{\fullwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}

\tikzstyle{every node}=[align=center, minimum width=1.25cm, minimum height=0.6cm]
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm, text width = 2
    cm, color = pruss}}

\node [pass] (VPA) {Value\\propagation};
\node [pass, right of = VPA] (AI) {Abstraction of instructions};
\node [pass, right of = AI] (BCP) {Constraint propagation};
\node [pass, fill=vivid!40, right of = BCP] (DI) {Domain insertion};

    \node [empty, left = 0.5 cm of VPA]  (in) {bitcode \\ + \\ domains};
\node [empty, right = 0.5 cm of DI]  (out) {abstracted bitcode};

\draw [flow] (in) -- (VPA);
\draw [flow] (VPA) -- (AI);
\draw [flow] (AI) -- (BCP);
\draw [flow] (BCP) -- (DI);
\draw [flow] (DI) -- (out);
\end{tikzpicture}
}

\caption{Sequence of transformation passes used for insertion of an arbitrary
    abstraction.}
\label{fig:transformation}
\bigskip
\prule
\end{minipage}}
\end{figure}

Abstraction of the program is done in four major steps. First, we analyze
wherever abstract values in the program may occur. This is done in value propagation
analysis. The computed set of instructions is then abstracted into intermediate
\LART instructions. Thereafter, \code{assume} calls are inserted into the
program and constraints are propagated through the bitcode. Finally, the
intermediate abstract representation is replaced by the appropriate operations
in the respective domain. First three analysis are domain independent and can be
done without the need for the actual domain implementation.

Providing the program and the domain implementation, one has
to annotate which variables should be abstracted into which domain. In
the following example, the variable \code{x} is annotated as symbolic.
\begin{minted}{cpp}
int foo(int a, int b) {
    _SYM int x;
    int y = 0;
    if ( x < a ) {
        y = x + 4;
    } else {
        y = bar(a, b);
    }
    y = bar(a, y);
    return y;
}
\end{minted}
In next sections, we work with this simple \Cpp{} program. For
readability we do not expose the corresponding \LLVM bitcode and instead
work schematically on control flow diagrams, on which the mapping to actual
\LLVM should be easily done. But for a curious reader, we attach results of
\LLVM transformation to the \autoref{ch:appendixa}. The corresponding control
flow graph for the example program is displayed in \autoref{fig:exampleprogram}.

\begin{example}\label{fig:exampleprogram}
Control flow graph of the example program. The green
boxes correspond to \LLVM basic blocks and the yellow diamond to conditional branching
using the result of a corresponding Boolean expression.

\begin{center}
\resizebox{.90\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {\_SYM i32 x };
    \node [llvm, below = 0.5 cm of declx] (decly) { i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [llvm, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [llvm, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [llvm, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5 pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\end{center}
\end{example}
In the program with annotations of abstract variables, we need to examine all the
reachable instructions from those variables. We call them the
\emph{roots} of the abstraction. The examination of the reachable instructions is
performed in the first step by \emph{value propagation analysis}~(VPA).

To generalise the process and to enable additional analysis, we
postpone abstract domain insertion until the entire program is analysed and
transformed into a suitable form.

Hence in the second step (after VPA), we introduce an intermediate representation of
abstract instructions. These instructions do not provide any implementation and
are represented a manner similar to \LLVM intrinsic functions
\cite{LLVM:langref}. During this phase (\emph{abstraction of instructions}) we
\emph{lift} the reached instructions provided by the VPA into the abstract form. The
only purpose of these intrinsics is to carry the domain information for further
analysis and to eliminate analysis of the domain implementation (since there is no need
to look into the implementation of domain operations). Additionally, the \LLVM
toolset can be used in this step for type checking of the applied transformation.

In the next analysis, \code{assume} calls are inserted and backwards constraint
propagation is executed. After all these analyses, the intermediate
representation of abstract operations is replaced by a real domain
implementation.

\subsection{Value propagation analysis}
In the first \LLVM pass, we analyse the reachable instructions from the
annotated variables. Reachable instructions are those that use abstract values
as their arguments. In this analysis, we also need to propagate through calls,
when abstract values are arguments of the called function. Similarly, analyse functions into which we can return an abstract value from some function. The
result of VPA is a list of functions with corresponding abstract roots.

\begin{example}\label{ex:vpa}
    When analyzing the function \code{foo} from \autoref{fig:exampleprogram}, we
    compute the reachable set of instructions using DFS from the root 
    \code{x}~through all its uses \cite{LLVM:langref}. We investigate all
    instructions where \code{x}~appears on the right-hand side (marked orange on
    the picture).

    The analysis find a \code{store} to variable \code{y} (in instruction the
    \code{y = x + 4}). Hence \code{y} is added to set of roots of the function
    \code{foo} and lifted into the domain that \code{x} belongs to. Further the
    analysis is repeated from the new root.
\begin{center}
\resizebox{.93\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [clllvm] (declx) {\_SYM i32 x };
    \node [llvm, below = 0.5 cm of declx] (decly) {i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [clllvm, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [llvm, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [llvm, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}}
\end{center}
\noindent
During the propagation analysis of \code{y}, we find that \code{y} is an
argument of the function \code{bar}, hence we need to analyze the function
\code{bar} and compute the roots for it. Resulting reachable instructions are illustrated below.
\begin{center}
\resizebox{.93\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [clllvm] (declx) {\_SYM i32 x };
    \node [clllvm, below = 0.5 cm of declx] (decly) { i32 y};
    \node [br, below = 0.5 cm of decly] (br) { x < a };
    \node [clllvm, below left = 0.5 cm of br] (brtrue) { y = x + 4 };
    \node [llvm, below right = 0.5 cm of br] (brfalse) { y = bar(a, b) };
    \node [clllvm, below = 1.5 cm of br] (retcall) { y = bar(a, y) };
    \node [clllvm, below = 0.5 cm of retcall] (ret) { ret y };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly)] (bb1) {};
        \node [bb, fit = (brtrue)] (bb2) {};
        \node [bb, fit = (brfalse)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32 foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (decly) -- (br);
    \draw [flow, very thick] (br.west) -| (brtrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (brfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}}
\end{center}
Note that even though the function
\code{bar} in the bottom basic block is called with an abstract argument, we want
the call to \code{bar} in the \code{else} branch to remain explicit.

Another reachable instruction worth mentioning is \code{ret}, since it means
that we need to change the signature of the function \code{foo} (returns an
abstract value). After the propagation of \code{y} we have not find any new abstract
variable, hence the analysis of the function \code{foo} is finished with the set
of roots $\{\code{x}, \code{y}\}$.
\end{example}

As described in \autoref{ex:vpa}, besides the exploring of reachable instructions, the value propagation analysis has to step into function calls with abstract arguments, marking the arguments as roots of the called function.

Since the called function may have some annotated values, we will distinguish
two types of roots: annotation-dependent and argument-dependent. Let us
illustrate the meaning of this distinction of roots using an example.

\begin{example} \label{ex:roots} Consider following two functions.
    \begin{minted}[linenos]{cpp}
int bar(int arg) {
    _SYM int y;
    int b = arg;
    if (b < y)
        return b;
    else
        return 42;
}

int foo() {
    _SYM int x;
    int val = 0;
    int res1 = bar(x);
    int res2 = bar(val);
}
\end{minted}
    We can see that variable \code{x} is a root for the function \code{foo} and
    \code{y} is a root for the function \code{bar}. Since \code{bar} is called
    with an abstract argument from \code{foo} (line 13.), we want to add an
    argument \code{arg} to the roots of \code{bar}. But in further analysis, we
    do not want to consider \code{bar} to be always called with an abstract
    value.

    Hence we distinguish two sets of roots for the \code{bar} function. One,
    when the function is called with a concrete value, which only includes the
    single root \code{y}. For the case when the argument is abstract, the
    propagation results into $\{\code{y}, \code{arg}, \code{b}\}$. Since
    \code{y} is always present in the set, because it is marked directly in the
    function, we represent the resulting roots as union of annotation-dependent
    roots $\{\code{y}\}$ and argument-dependent roots $\{\code{arg},
    \code{b}\}$.
\end{example}

As illustrated in \autoref{ex:roots}, the result of value propagation analysis
is, for each function, a set of annotation-dependent roots and sets of
argument-dependent roots.

Additionally, you may have noticed that depending on the
argument, the result of the function \code{bar} can be abstract. To record this
information, we also add the resulting value \code{res1} to anno\-tation-dependent
roots of the function \code{foo}, since the abstract call was made due to the propagation of its annotation roots.

Because we need to determine the return type of each function precisely, we
utilize the \code{UnifyFunctionExitNodes} pass, provided by the \LLVM toolset,
to unify all exit nodes to a single one (resulting function contains a single
\code{ret} instruction). But when the function may return a value of multiple
domains (see the function \code{bar} in the previous example) we need to put all
the values to the same domain. The transformation can be performed by a simple
lift of concrete values to the corresponding abstract domain. In the function
\code{bar}, we lift the value 42 to the symbolic domain.

Besides 'downwards' propagation (in the direction of control flow), we need
to handle backwards propagation. This case occurs when some abstract value is
stored into a nonabstract output argument (pointer or reference) of the function,
illustrated below.
\begin{minted}{cpp}
void init(int *x) {
    _SYM int v;
    *x = v;
}
int main() {
    int x;
    init(&x);
}
\end{minted}

To handle the backwards propagation, we mark the output argument as a root of the
function, and examine all the locations from where the function is called. From
the call locations we compute the origin of the argument and put it into the
domain that was stored to it (i.e. variable \code{x} is a symbolic root
in the function \code{main}).

Last of all, we need to handle abstraction of aggregate types. Since we enable
only annotation of scalar types, abstract elements in aggregates may occur
only due to stores. The problem to be solved is how to mark domains of a
particular element of the aggregate. For this purpose, we introduce a tree-like
structure, that mimics the structure of the aggregate type and stores domains of
its scalar elements (shown in \autoref{ex:fieldtrie}).

In \LLVM, when an aggregate is created by an \code{alloca} instruction, we get a
pointer to the aggregate base. In order to access the elements, we need to
\code{load} using the pointer to the aggregate. The specific element is then
accessed using the \code{getelementptr} instruction, which computes for a given
sequence of indices, an offset into the structure.

\begin{example} \label{ex:fieldtrie}
Consider a nested structure represented in \LLVMIR:
\begin{minted}{llvm}
%Widget = type { i64, %Store* }
%Store = type { i64, i64, i64 }
\end{minted}

We have utilised this indexing strategy to create a tree-like representation
of types.\sidenote{The tree representation is sufficient only in simple cases.
For more complex structures (recursive data types) we need to employ a shape
analysis.} The representation of \code{Widget} as a tree is shown in the
left picture. The edges of the tree represent the access operation type
(i.e \code{load} operation or index in \code{getelementptr} instruction)
\cite{LLVM:langref}.

Since we work only with the abstraction of scalar types, we need to focus only on
the leafs of the tree structure. To denote a domain, we assign the type of the
abstraction to the leaf. Because we only need information about abstract
elements, we do not store the rest of the leafs. Example of an abstract structure
with multiple domains is in the right picture.

\begin{center}
\begin{minipage}[t]{.47\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm, level
    distance = 1cm}]
    \tikzstyle{fieldnode} = [
        ftreenode,
        minimum width = 0.2cm,
        minimum height = 0.2cm
    ];
    \node [fieldnode, label=180:{\small\code{Widget*}}] {}
        child { node [fieldnode, label=180:{\small\code{Widget}}] {}
            child{ node [fieldnode] {}
                child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                edge from parent node [left, above=1pt] {\small\code{0}} }
                child{ node [fieldnode, label=0:{\small\code{Store*}}] {}
                    child{ node [fieldnode, label=0:{\small\code{Store}}] {}
                        child{ node [fieldnode] {}
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [left, above=1pt] {\small\code{0}} }
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [left] {\small\code{1}} }
                            child{ node [fieldnode, label=180:{\small\code{i64}}] {}
                            edge from parent node [right, above=1pt] {\small\code{2}} }
                        edge from parent node [left] {\small\code{0}}
                        }
                    edge from parent node [left] {\small\code{load}}
                    }
                edge from parent node [right, above = 1pt] {\small\code{1}}
                }
            edge from parent node [left] {\small\code{0}}
            }
        edge from parent node [left] {\small\code{load}}
        }
    ;
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{.47\textwidth}
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 2cm, level
    distance = 1cm}]
    \tikzstyle{fieldnode} = [
        ftreenode,
        minimum width = 0.2cm,
        minimum height = 0.2cm
    ];
    \node [fieldnode, label=180:{\small\code{Widget*}}] {}
        child { node [fieldnode, label=180:{\small\code{Widget}}] {}
            child{ node [fieldnode] {}
                child{ node [fieldnode, label=180:{\color{orioles}zero}] {}
                edge from parent node [left, above=1pt] {\small\code{0}} }
                child{ node [fieldnode, label=0:{\small\code{Store*}}] {}
                    child{ node [fieldnode, label=0:{\small\code{Store}}] {}
                        child{ node [fieldnode] {}
                            child{ node [fieldnode, label=0:{\color{orioles}symbolic}] {}
                            edge from parent node [left] {\small\code{1}} }
                        edge from parent node [left] {\small\code{0}}
                        }
                    edge from parent node [left] {\small\code{load}}
                    }
                edge from parent node [right, above = 1pt] {\small\code{1}}
                }
            edge from parent node [left] {\small\code{0}}
            }
        edge from parent node [left] {\small\code{load}}
        }
    ;
\end{tikzpicture}
\end{minipage}
\end{center}
\end{example}

During value propagation analysis, we store these trees for each variable.
To be consistent, we represent scalar types in the same way (i.e. they are
represented as a single node). For pointer types, the trees look like a chain of
\code{load} edges.

\begin{summary}
In summary, the value propagation analysis takes annotated \LLVM bitcode and
returns the roots of abstraction. Roots are those \LLVM values that introduce an
abstract value in the context of a single function. We have divided roots into two
kinds: annotation-dependent, that are dependent directly on annotated values
in the function, and argument-dependent, that are dependent
on the abstracted arguments of the function. At the end of the section, we
have shown how to analyze aggregate types with multiple domains using a tree
representation of the structures.
\end{summary}

\subsection{Lifting of instructions}
Having the abstraction roots from the value propagation analysis, we can
start program transformation. In this \LLVM pass, we want all the instructions
that work with abstract values to be replaced by abstract intrinsics.
Additionally, all the required function prototypes have to be created, and
appropriate abstract implementation inserted into them.

From the abstraction roots computed in VPA, we can perform the transformation per
function. But first of all, the function prototypes are created. This is performed because we may need to call some abstract function during the transformation, which has not been abstracted yet. But to create a call to a function, its prototype is sufficient. This approach also elegantly deals with recursive calls.
\begin{marginfigure}
\itshape
Consider a recursive function:
\begin{minted}{cpp}
int foo(int a) {
  if (a == 0)
    return 0;
  else
    return foo(a - 1);
}
\end{minted}
    When the argument \code{a} is abstract, we need to create a prototype of
    the function \code{foo} with an abstract argument before we start the
    transformation of the function, because during the transformation we need to
    call the function \code{foo} with an abstract argument.
\end{marginfigure}

Function prototypes are created based on the abstraction roots. Basically
the transformation iterates over all the sets of argument-dependent roots and
the prototype is created based on the arguments included in the given set.
Moreover, the return type is deduced from the reachability of the \code{ret} instruction
from the roots.

In order to lift \LLVM instructions and function prototypes, we need to
establish an abstract type system. To do so, we express an abstract type as an
\LLVM structure, which maintains the information about the domain and original
type. For example, an integer in the symbolic domain is represented as follows.

\begin{minted}{llvm}
%lart.sym.i32 = type { i32 }
\end{minted}

Names of abstract types are always prefixed by \code{lart}, followed by the
domain name and finished by the name of a lifted \LLVM type.

In this type system, we can create desired prototypes for the functions from our
example program (\autoref{fig:exampleprogram}). Expecting that the function \code{foo}
does not take any abstract value we need only to change to return type of the
signature. For the function \code{bar} there are two cases: one does not take any
abstract value and the second one has one abstract argument. After prototype
creation, the resulting \LLVM declarations look like this.
\begin{minted}{llvm}
declare %lart.sym.i32 @foo.2(i32, i32)
declare %lart.sym.i32 @bar.2(i32, %lart.sym.i32)
\end{minted}
Since \LLVM requires distinct names for each function, indices are added in
the order of prototype creation. The transformation is performed per function,
specifically for each set of roots separately.

From given roots, we compute the reached set of instructions and sequentially
transform them. In order to have all the instructions transformed before their
result is used further in the function, we transform them in a reverse postorder manner.

We represent abstract operations as function calls, with similar naming
conventions as \LLVM-native intrinsic operations \cite{LLVM:langref}. The
operations still need to keep track of the domain type and the base \LLVM instruction.
For example for the \code{add} operation, we create an abstract equivalent:
\begin{minted}{llvm}
%res = call %lart.sym.i32
            @lart.sym.add.i32(%lart.sym.i32 %a,
                              %lart.sym.i32 %b)
\end{minted}

When an instruction takes both a concrete and abstract argument, we need to
transform the concrete value into the correct domain. To do so, we perform an
operation \code{lift}, which creates an abstract constant (described in
\autoref{sec:absdom}).

Besides within-domain operations\sidenote{Within-domain operations are those
that take and return values from the same domain.}, we need to solve interactions between domains.
As mentioned in \autoref{sec:interactions}, we need to transform in-domain
Booleans into tristate manipulations. We do this to unify control flow
handling when dealing with multiple domains. Let us consider the following
condition to be abstracted:
\begin{minted}{llvm}
%cond = icmp sgt i32 %a, %b
br i1 %cond, label %then, label %else
\end{minted}
Since we do not know without further analysis whether the resulting value of
\code{icmp} is used in control flow determination, we prefer the result to
remain in the abstract domain. Only when we found a use of abstract Boolean in
some control flow instruction (i.e. branching), we insert a conversion operation
from the abstract Boolean to tristate (described in \autoref{sec:interactions}).
Immediately we lower the tristate into a concrete Boolean in order to preserve
\marginpar{During execution, the lowering of tristate may cause a non
deterministic choice if its value is uncertain i.e.~\emph{maybe}.}
explicit control flow:
\begin{minted}{llvm}
%cond = call %lart.sym.i1 @lart.sym.icmp_sgt.i32(%lart.sym.i32 %a,
                                                 %lart.sym.i32 %b)
%tris = call %lart.tristate
             @lart.sym.bool_to_tristate(%lart.sym.i1 %cond)

%bool = call i1 @lart.tristate.lower(%lart.tristate %tris)
br i1 %bool, label %then, label %else
\end{minted}

\begin{example}
With created prototypes the whole transformation of the program from
\autoref{fig:exampleprogram}:
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {i32\textsubscript{sym} \textoverline{x}};
    \node [llvm, below = 0.2 cm of declx] (decly) {i32\textsubscript{sym}
    \textoverline{y}};
    \node [llvm, below = 0.2 cm of decly] (lifta) {\textoverline{a} = lift\textsubscript{sym}(a)};
    \node [llvm, below = 0.2 cm of lifta] (cond) { \textoverline{cond} = \textoverline{x} <\textsubscript{sym}
    \textoverline{a} };
    \node [br, below = 0.5 cm of cond] (br) { lower(\textoverline{cond}) };
    \node [llvm, below left = 0.5 cm of br] (lift4) { \textoverline{4} =
    lift\textsubscript{sym}(4) };
    \node [llvm, below = 0.2 cm of lift4] (brtrue) { \textoverline{y} =
    \textoverline{x} +\textsubscript{sym} \textoverline{4} };
    \node [llvm, below right = 0.5 cm of br] (liftbar) { y = bar(a, b) };
    \node [llvm, below = 0.2 cm of liftbar] (brfalse) { \textoverline{y} =
    lift\textsubscript{sym}(y) };
    \node [llvm, below = 2.5 cm of br] (retcall) { \textoverline{y} =
    \textoverline{bar}(a, \textoverline{y}) };
    \node [llvm, below = 0.2 cm of retcall] (ret) { ret \textoverline{y} };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly) (lifta) (cond)] (bb1) {};
        \node [bb, fit = (brtrue) (lift4)] (bb2) {};
        \node [bb, fit = (brfalse) (liftbar)] (bb3) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32\textsubscript{sym} foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (cond) -- (br);
    \draw [flow, very thick] (br.west) -| (lift4.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (liftbar.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}
\end{center}

In the picture, symbolic values are represented by a bar over them.
A notable change in the program are points where concrete values have to be lifted to the symbolic domain. Besides that, a transformation of conditional branching occurs, with an explicit branch over the lowered tristate. Lastly, the
correct symbolic form of \code{bar} function has to be called. We can observe that call
with concrete arguments remains untouched.
\end{example}

When dealing with aggregate or pointer types, we know that they are not
abstract, so we do not need to create abstract operations, that manipulates
them. Hence the original instructions manipulating aggregates remain
untouched, except the instructions that extract or insert scalars to them.

In those situations, we take the use of results from the value propagation analysis.
Knowing which elements of an aggregate are abstract, when the program tries to
reach some of them (through some pointer) we just simply bitcast
the resulting concrete pointer to a pointer to an abstract element. We can afford
to do this because we know that in the particular memory location, an abstract value is
stored (see \autoref{ex:storeagg}).

\begin{example}\label{ex:storeagg}
An access to an aggregate type may look as follows (\Cpp{} on the left side and interesting
part of the corresponding \LLVM on the right side):

\bigskip
\noindent
\adjustbox{valign=t}{
\begin{minipage}{.4\textwidth}
\begin{minted}{cpp}
struct S { long a; };

int main()
{
    S s;
    _SYM long v;
    s.a = v;
}
\end{minted}
\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}{.55\textwidth}
\begin{minted}{llvm}
%S = type { i64 }
...
%s = alloca %S
%v = alloca i64
%a = getelementptr %S, %S* %s,
                   i32 0, i32 0
%l = load i64, i64* %v
store i64 %lv, i64* %a
\end{minted}
\end{minipage}}

\bigskip
\noindent
In the \LLVM, the structure \code{s} is accessed via \code{getelementptr}
instruction. This instruction computes an offset to the element \code{a} of
the structure \code{s}. If we want to store a symbolic value \code{v} to
the \code{a} we need to bring \code{a} to the same domain. We perform this
by \code{bitcasting} of \code{a} to the symbolic value. After the
transformation the \LLVM bitcode looks like this:
\begin{minted}{llvm}
%S = type { i64 }
...
%s = alloca %S
%v = call %lart.sym.i64* @lart.sym.alloca.i64()
%a = getelementptr %S, %S* %s, i32 0, i32 0
%b = bitcast i32* to %lart.sym.i64*
%l = call %lart.sym.i64 @lart.sym.load.i64( %lart.sym.i64* %v )
call void @lart.sym.store.i64( %lart.sym.i64 %l, %lart.sym.i64* %b )
\end{minted}
\end{example}

However, this approach has its limitations, because the abstract representation
has to fit into the size of the original type in the aggregate (in the example
abstract \code{v} had to fit into the size of element \code{a}). As consequence
we are not currently able to store a pointer to the representation of an abstract type
into a smaller type than the pointer type (\code{i64} on 64-bit
architecture).\sidenote{This limitation can be removed by a table during
runtime, in which program stores information about abstract types.}

But a significant advantage of this approach is that we do not need to create
abstract aggregate types and handcraft entire structures. This way we also
preserve pointer arithmetic operations, since the offsets within
aggregates do not change.

\begin{summary}
Given the roots of abstraction, we have shown how to transform a program into an
abstract intermediate form. Firstly we have constructed prototypes of
functions with abstract signature (either has some abstract argument or
returns an abstract value). Further, we have transformed all instructions
reachable from the abstraction roots into abstract intrisics. Accordingly, we
have introduced a representation of abstract types. During the transformation
process, we have also solved an interaction with control flow using
transformation to tristate and lowering to \LLVM Boolean.
\end{summary}

\subsection{Backwards constraint propagation and value restriction}
\label{sec:bcp}
When a branch is taken due to a lowered tristate, we need to restrict the values
engaged in the condition. Consider that a branch is taken, based on comparison $a
> 10$, where $a$ is an abstract value. If we do not have any restriction on $a$
yet and the branch was taken nondeterministically, as a consequence, the value of $a$
has to be greater than 10. However, when the branch was not taken,
the value has to be 10 or less. We utilise this fact to restrict the value
right after the branch was taken.

We can compute the value restrictions independently of the abstract domain, as
long as the abstract domain provides the right set of primitives. Specifically, we
need to be able to obtain a new restricted value from a value and a predicate
instruction on which we want to base the restriction, along with the actual
result of the predicate. For this purpose, we have established
an \code{assume} operation.
\begin{minted}{llvm}
%x = call %lart.sym.i32
          @lart.sym.assume(%lart.sym.i32 %a,
                           %lart.tristate %cond,
                           i1 true)
\end{minted}
Meaning that value \code{\%x} carries the restricted value of \code{\%a}
according to the satisfied condition \code{\%cond}.

The insertion of assumes is performed in two steps: first, we restrict only the
tristates that are directly lowered and used in conditional branching. By this
transformation, we simply denote which path is taken based on the nondeterministic
choice. We then proceed in restriction of variables that are engaged in the condition
predicate. It may seem reasonable to carry on backwards in the program history to
provide more precise restriction, but backwards constraint propagation has some
limitations.

Since the whole computation has to be performed statically, it is not clear how far the
backwards lookup should proceed. The other limitation is that the propagation
can only sufficiently analyse only the local static scope and cannot infer
information from function arguments and global state.

Consequently, we need to make a trade-off between work given to domain
implementation and simple constraint analysis. In the symbolic domain, we can achieve restrictions by building a path condition. Hence we do not need the
complicated analysis, in this case, so we have decided to retain only simple
assume insertion algorithm for this thesis.

The symbolic domain can simply collect the restrictions as path condition and
restricts the possible variable evaluations based on the entire path
(detailed description of utilisation of assumes in symbolic domain is
described in \autoref{sec:symbolic}).

In addition to backwards analysis, assumption demands a few minor
modifications to bitcode. When control flow of restricted value merges with
control flow from another restriction or original definition, we need to insert
appropriate $\varphi$ nodes.

For simpler dependency analysis and $\varphi$ node insertion, we have decided
to insert the restrictions on the edge between the restricting branch and the consequent
basic block (illustrated in the \autoref{ex:assumes}).

\begin{example} \label{ex:assumes}
Analyzing the example program (from \autoref{fig:exampleprogram}), we need to
restrict values depending on one conditional branching:
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[>=stealth',shorten >=1pt,auto,node distance=4em, <->]
\tikzset{>=latex}
\tikzset{empty/.style = {minimum width=0cm,minimum height=1cm}}
    \node [llvm] (declx) {i32\textsubscript{sym} \textoverline{x}};
    \node [llvm, below = 0.2 cm of declx] (decly) {i32\textsubscript{sym}
    \textoverline{y}};
    \node [llvm, below = 0.2 cm of decly] (lifta) {\textoverline{a} = lift\textsubscript{sym}(a)};
    \node [llvm, below = 0.2 cm of lifta] (cond) { \textoverline{cond} =
    \textoverline{x} <\textsubscript{sym} \textoverline{a} };
    \node [br, below = 0.5 cm of cond] (br) { lower(\textoverline{cond}) };
    \node [clllvm, below left = 0.5 cm of br] (asstrue)
    {\textoverline{x} = assume(\textoverline{x} <\textsubscript{sym} \textoverline{a})};
    \node [llvm, below = 0.5 cm of asstrue] (lift4) { \textoverline{4} =
    lift\textsubscript{sym}(4) };
    \node [llvm, below = 0.2 cm of lift4] (brtrue) { \textoverline{y} =
    \textoverline{x} +\textsubscript{sym} \textoverline{4} };
    \node [clllvm, below right = 0.5 cm of br] (assfalse)
    {\textoverline{x} = assume(\textoverline{x} >=\textsubscript{sym}
    \textoverline{a})};
    \node [llvm, below = 0.5 cm of assfalse] (liftbar) { y = bar(a, b) };
    \node [llvm, below = 0.2 cm of liftbar] (brfalse) { \textoverline{y} =
    lift\textsubscript{sym}(y) };
    \node [llvm, below = 4 cm of br] (retcall) { \textoverline{y} =
    \textoverline{bar}(a, \textoverline{y}) };
    \node [llvm, below = 0.2 cm of retcall] (ret) { ret \textoverline{y} };

    \begin{pgfonlayer}{background}
        \node [bb, fit = (declx) (decly) (lifta) (cond)] (bb1) {};
        \node [bb, fit = (brtrue) (lift4)] (bb2) {};
        \node [bb, fit = (brfalse) (liftbar)] (bb3) {};
        \node [bb, fit = (asstrue)] (bbt) {};
        \node [bb, fit = (assfalse)] (bbf) {};
        \node [bb, fit = (ret) (retcall)] (bb4) {};
        \node [fun, label={ i32\textsubscript{sym} foo(i32 a, i32 b)}, fit = (bb1) (bb2) (bb3) (bb4)] (fn) {};
    \end{pgfonlayer}

    \draw [flow, very thick] (cond) -- (br);
    \draw [flow, very thick] (asstrue) -- (lift4);
    \draw [flow, very thick] (assfalse) -- (liftbar);
    \draw [flow, very thick] (br.west) -| (asstrue.north) node [near start, above = 5pt] {true};
    \draw [flow, very thick] (br.east) -| (assfalse.north) node [near start, above = 5pt] {false};
    \draw [flow, very thick] (brtrue.south) |- (retcall.west);
    \draw [flow, very thick] (brfalse.south) |- (retcall.east);
\end{tikzpicture}
}

In this simple case, proper assumes are inserted on the edges between branching
and consequent basic blocks. Remark that new basic blocks were created for
each assume. Creation of new basic blocks does not make any difference in this
program, but for more complicated functions with loops, edge splitting needs
to be done to enable $\varphi$ nodes creation.

Notice, that each assume returns a new value for the restricted variable, which
is used in the following code.  After an inspection we may observe that assume
in \emph{false} branch is unnecessary since the value of restricted
\code{\textoverline{x}} is never used afterwards, but we do not perform any
optimisations in this case.
\end{example}

\begin{summary}
When an abstract program executes a nondeterministic choice on a branch, it
needs to constrain abstract values that appeared in the condition of the
branching. For this purpose, we have introduced \code{assume} calls. Given
the abstract value, the condition predicate, and the result of the
predicate, an \code{assume} call computes the constrained value. In this
section, we have also introduced backwards constraint propagation, which
provides more precise restrictions.
\end{summary}

\subsection{Domain insertion}

After all the analyses and transformations are finished, we can proceed to
actual domain insertion. As a result of previous transformations we have all
abstract instruction already in the bitcode, and we just need to replace them by
their implementation in the domain. Since no further analysis is required, we do
the substitution per instruction.

Like in the abstraction of instructions from the second phase, we need to
substitute instructions before they are used further in the bitcode (we proceed
in reverse postorder). When processing an instruction, we look at the domain tag
(in the name of the abstract intrinsic) and accordingly choose the appropriate
implementation. Currently, abstract intrinsics are replaced by calls to functions
implementing the domain operations. This may be optimised in the future as
inlining of a block of code or direct replacement by a by single instruction
(possible for simpler domain operations), to minimise the number of calls in the
bitcode.

From the domain implementation point of view, there are two things to be
provided. First, the implementation of operations in the domain, including
the special ones -- \code{assume}, \code{lift} and \code{bool\_to\_tristate}.
And secondly, the implementation of lowering function that, given an abstract
\LART intrinsic and its arguments, produces a corresponding call to a domain
operation.

The limitation of a transformation is that the abstract values must have a
run-time representation using scalar values from the concrete domain. This is
not a problem for simpler domains like $Z$, but for symbolic abstraction, the
value representation has to be tweaked. As proposed in \autoref{sec:sym}, the
data have to be represented by a formula tree. This representation can be
then stored behind a pointer, and stored on program heap (i.e.~abstract value in the program is a pointer to its representation). But with this approach, we
introduce memory leaks to the transformed bitcode. The other way is to copy the symbolic
representation on every operation, which is quite inefficient at runtime. Due to
this, we have used special pointers provided by \DIVINE (weak pointer), which
\DIVINE recognize as not leaking. These pointers are then maintained by \DIVM similarly as garbage collectors do. Hence during the verification, \DIVINE
distinguishes the pointers to symbolic values and conceal the memory leak.

For verification in \DIVINE, weak pointers are also crucial for the state
comparison, since we do not want to compare the shape of the tree behind them.
The exact comparison is described in the next section.

\begin{summary}
In this section, we have shown, how the actual domain is inserted into the
program. Insertion is performed in a per-instruction manner: given an abstract
instruction (intermediate intrinsic call), we replace it by corresponding code
that implements manipulation in a given domain. We have also illustrated
problems with more complicated data representation.
\end{summary}

\section{Symbolic state space exploration} \label{sec:symbolic}

When the program transformation is finished, abstracted program is given to
\DIVINE for model checking. \DIVINE then interprets the program, in which
abstract operations manipulate symbolic data. Moreover, a path condition is
created during the execution of assumes. However, equality and reachability of
symbolic states cannot be decided explicitly. Therefore we need to
employ an exploration algorithm similar to
\SymDIVINE (see \autoref{sub:symdivine}).

During interpretation of a program, we need to check whether the state is
reachable. This is done by querying an \SMT solver, to decide if the path
condition is satisfiable. In current implementation, a path condition is
represented by a global formula, which is edited each time an assume is called.
When the path condition changes, we need to check if it still satisfiable, hence
proof a reachability of the state. We discuss a complete state space
exploration process in the following section.

\subsection{Exploration algorithm}
The symbolic exploration algorithm, similarly as explicit algorithm, uses \DIVM
to interpret the \LLVM instructions. During the interpretation, abstracted
operations manipulate with symbolic values, according to the inserted implementation of
symbolic domain (i.e.~they operates on formula trees, see \autoref{sec:sym}).
Note that \DIVM interprets the symbolic representation as an ordinary \LLVM bitcode.

In order to create path condition, a program needs to track constraints over the
symbolic values. This is achieved with \code{assume} calls inserted after the
control flow branching. Look at the simple program with some branches:
\begin{minted}[linenos]{cpp}
__SYM int x;
if (x < 10) {
    if (x > 5) {
        ...
    }
}
\end{minted}
In the program above on the 4.~line, we need to ensure that value of \code{x} is
between 5 and 10. This has to be done because the comparison of symbolic values
does not restrict the original value, only creates a nondeterministic choice
whether the branch is taken or not, hence the value of \code{x} is arbitrary even if
we have decided that it is smaller than 10 in the first condition.

The restriction is achieved, right after the branch is taken, by
\code{assume} call. To maintain the restrictions, a path condition is stored in
some global data structure. Then, when the \code{assume} call is executed a
restriction is appended to the path condition. This is done in the
implementation of the call \code{assume} in the symbolic domain
\sidenote{Manipulations with the path condition are interpreted by \DIVM.}. Looking back to
the example, the path condition on the 4.~line is represented by two
restrictions: $(x < 10)$ and $(x > 5)$.

When a new state is generated, we need to check whether the state is reachable.
We do an equality check similarly as in \SymDIVINE, by checking the satisfiability of the
path condition. The path condition for \SMT query is created as conjunction of
all restrictions taken so far, i.e.~for line 4, the path condition looks like:
\[ (x < 10) \wedge (x > 5).\]
Since the path condition is changed only when the \code{assume} call is
interpreted, it is not necessary to check reachability for every state, but only
for those when the path condition changes, i.e.~after the \code{assume} call.

\subsection{Equality check}

When we know that state is reachable, we need to check whether we have not seen
this state already. The comparison needs to be done with all visited states on
the same control flow location. Similarly as in \SymDIVINE (see
\autoref{subsec:equality}), we compare the explicit part of the
states and then the symbolic part using the \SMT solver.

Let's have two states $s_1$ and $s_2$, in the same control flow location, with
the same memory shape. This is when $s_1 = (c, m, \varphi)$ and $s_2 = (c,
m, \psi)$ for control location $c$, memory shape $m$ and path conditions $\varphi$ and $\psi$.
\sidenote{Note that, formula-trees does not count into the comparison of memory
shape, because they are stored behind marked pointers, hence \DIVINE does not
consider them.}
For example representation of the memory configuration of compared states (in
the same control location) may look like this:

\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 1.3cm, level
    distance = 1cm}]

    \tikzset{state/.style = {draw, inner sep = 5pt,rectangle split, rectangle split horizontal,
    rectangle split parts=6}, minimum height = 0.8cm}
    \node [state, label={$s_1$}] (s1)
        {$c_1$
         \nodepart{two} \dots
         \nodepart{three} $p_1$
         \nodepart{four}~~~~\dots~~~~
         \nodepart{five} $p_2$
         \nodepart{six} \dots};
    \node [state, right = 1 cm of s1, label={$s_2$}] (s2)
        {$\widehat{c}_1$
         \nodepart{two} \dots
         \nodepart{three} $\widehat{p}_1$
         \nodepart{four} ~~~~\dots~~~~
         \nodepart{five} $\widehat{p}_2$
         \nodepart{six} \dots};
    \node [below = 0.5 cm of s1.three south, ftreenode] (t1) {$+$}
        child{ node [ftreenode] {$v_1$} }
        child{ node [ftreenode] {$1$} }
    ;

    \node [below = 0.5 cm of s1.five south, ftreenode] (t2) {$*$}
        child{ node [ftreenode] {$v_2$} }
        child{ node [ftreenode] {$v_3$} }
    ;

    \node [below = 0.5 cm of s2.three south, ftreenode] (t3) {$+$}
        child{ node [ftreenode] {$+$}
            child { node [ftreenode] {$v_1$} }
            child { node [ftreenode] {$1$} }
        }
        child{ node [ftreenode] {$1$} }
    ;

    \node [below = 0.5 cm of s2.five south, ftreenode] (t4) {$*$}
        child{ node [ftreenode] {$v_2$} }
        child{ node [ftreenode] {$v_3$} }
    ;

    \draw [->] (s1.three south) -- (t1);
    \draw [->] (s1.five south) -- (t2);
    \draw [->] (s2.three south) -- (t3);
    \draw [->] (s2.five south) -- (t4);
\end{tikzpicture}
}
\end{center}

States in the same control location, do not differ in the memory shape. We can
see that concrete values ($c_1$, $\widehat{c}_1$) and symbolic values ($p_1,
p_2, \widehat{p}_1, \widehat{p}_2$) occupy the same memory locations in both
states. Symbolic values are represented by formula trees, as described in
\autoref{sec:sym}. In the leafs of trees, we maintain constants or symbolic
variables.  We remark that, even though symbolic values are represented by tree,
we do not consider them in the memory shape comparison. Only the position of
symbolic values $p_k$ is considered.\sidenote{Rest of the formula tree is hidden
behind a special pointer in \DIVINE, which is not included in the memory shape
comparison.}

Semantically, we consider input values to come from a single source. As they are
encountered during the interpretation, we number them in linear order. Hence,
when comparing two states, input values are paired according to this
given order.\sidenote{In this way, we distinguish when a parallel program loads
two values in different order, even though it does not depend on the order of
the loads.} We denote symbolic variables of both states as $v_1, \dots, v_n$.
The values $p_1, \dots, p_k$ in the state $s_1$ and $\widehat{p}_1, \dots,
\widehat{p}_k$ in state $s_2$ are results of operations on the symbolic values.
From the logic point we consider them as nullary functions, these functions are
predicates over the symbolic values. For example $p_1$ defines a nullary
function $v_1 + 1$.

Comparing these states, we compare the explicit values directly (i.e.~$c_i =
\widehat{c}_i$ for all the explicit values in the states).  When explicit parts
are same we need to compare the symbolic parts. To decide whether the sets of
concrete states represented by $s_1$ and $s_2$ are equal, we define a formula
$equals(s_1, s_2)$:
\begin{equation*}
\begin{aligned}
    pc_{eq}(\varphi, \psi) \defeq & ~(\varphi \iff \psi) \\
    val_{eq}(s_1, s_2) \defeq & ~\varphi \implies (p_1 = \widehat{p}_1 \wedge \dots \wedge p_k = \widehat{p}_k ) \\
    equals(s_1, s_2) \defeq & ~\forall~v_1,\dots, v_n (pc_{eq}(\varphi, \psi)
    \wedge val_{eq}(s_1, s_2))
\end{aligned}
\end{equation*}
The formula says, that states are equal when the path conditions are
\marginpar{In reality when we check for equality of two states, we just need to
find the proof that there exists an evaluation distinguishing both states.
This is done easily by querying the \SMT solver for $\neg equals(s_1, s_2)$.
Hence we have simplified the work for solver, because it has to find a proof only
for existential quantifier used in negated formula instead of universal from
original formula.}
equivalent (defined by $pc_{eq}$) and data definitions describe the same set
of data valuations (defined by $val_{eq}$). We check for data equality only in
the case when path conditions are satisfiable. Since the path conditions must
be equivalent, it does not matter which one we check for satisfiability.

Equality of data valuations is defined in per value manner. We couple the
corresponding symbolic values defined by nullary functions and compare them for
equality. These equalities have to be satisfied for all free variables (symbolic
inputs).

From the example, we investigate whether $p_1 = \widehat{p}_1 \wedge p_2 =
\widehat{p}_2$ for all possible values of symbolic variables. Expanding nullary
functions we get $(v_1 + 1) = ((v_1 + 1) + 1) \wedge (v_2 * v_3) = (v_2 * v_3)$. Hence
we ask the \SMT solver whether this formula is satisfiable for all possible
values of $v_1, v_2, v_3$.

Hence to check whether \DIVINE encounters a new state $s$, we query the \SMT solver
for equality with all already reached states in the same control location with the same memory shape.

In comparison to \SymDIVINE, we create simpler queries to an \SMT solver.  This
is because our equality check contains only one implicit existential quantifier
in compare to an alternation of two quantifiers used in \SymDIVINE formula
$\notsubseteq$ (see \autoref{subsec:equality}). The difference is created by
numbering on the input values in \DIVINE, which match the corresponding inputs
during the equality check. Whereby \SymDIVINE do not utilise numbering on the
inputs and checks all possible pairings of them (using a quantifier in a
formula). In this way, \SymDIVINE can merge more states, but for the cost of
more expensive equality checks.


